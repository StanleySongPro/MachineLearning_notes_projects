{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "**[`1.Reference`](#1)**<br>\n",
    "**[`2.NLTK Tutorial`](#2)**<br>\n",
    "**[`4.Dive Into NLTK, Part V: Using Stanford Text Analysis Tools in Python`](#4)**<br>\n",
    "[**`5.Named Entity Recognition with NLTK and SpaCy`**](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.References<a name = '1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [NLTK Tutorials](https://www.nltk.org/book/ch01.html)\n",
    "* [Deep Learning for NLP: An Overview of Recent Trends](https://medium.com/dair-ai/deep-learning-for-nlp-an-overview-of-recent-trends-d0d8f40a776d)\n",
    "* [Tokenizing Words and Sentences with NLTK](https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/)\n",
    "* [You (Probably) Don‚Äôt Need For-Loops](https://medium.com/python-pandemonium/never-write-for-loops-again-91a5a4c84baf)\n",
    "* [Categorizing and Tagging Words](https://www.nltk.org/book/ch05.html)\n",
    "* [How machines understand our language: an introduction to Natural Language Processing](https://towardsdatascience.com/how-machines-understand-our-language-an-introduction-to-natural-language-processing-4ab4bcd47d05)\n",
    "* [Natural Language Processing is Fun! Part I](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)\n",
    "* [Natural Language Processing is Fun! Part II: Text Classification is Your New Secret Weapon](https://medium.com/@ageitgey/text-classification-is-your-new-secret-weapon-7ca4fad15788)\n",
    "* [Parsing English in 500 Lines of Python](https://explosion.ai/blog/parsing-english-in-python)\n",
    "* [State-of-the-art neural coreference resolution for chatbots](https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30)\n",
    "* [Recurrent Neural Networks: The Powerhouse of Language Modeling\n",
    "](https://towardsdatascience.com/recurrent-neural-networks-the-powerhouse-of-language-modeling-d45acc50444f)\n",
    "* [Machine Learning tackles the Fake News problem](https://towardsdatascience.com/machine-learning-tackles-the-fake-news-problem-c3fa75549e52)\n",
    "* [üìöThe Current Best of Universal Word Embeddings and Sentence Embeddings\n",
    "](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)\n",
    "* [Regular Expressions with re Python Tutorial](https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/)\n",
    "* [How do I update Anaconda?](https://stackoverflow.com/questions/45197777/how-do-i-update-anaconda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.NLTK Tutorial<a name = '2'></a>\n",
    "* Ê†áËØÜÂåñÂ§ÑÁêÜ\n",
    "* ËØçÂπ≤ÊèêÂèñ\n",
    "* ËØçÊÄßËøòÂéü\n",
    "* ÂÅúÁî®ËØçÁßªÈô§„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Words and Sentences with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? \\\n",
    "The weather is great, and Python is awesome. The sky is \\\n",
    "pinkish-blue. You shouldn't eat cardboard.\"\n",
    "\n",
    "print(sent_tokenize(EXAMPLE_TEXT))\n",
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Stop words with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = stopwords.words('English')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'Python', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "filtration_words = [w for w in word_tokenize(EXAMPLE_TEXT) if w not in stop_words]\n",
    "print(filtration_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words with NLTK ËØçÊ†πÊèêÂèñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'import', 'to', 'by', 'veri', 'pythonli', 'while', 'you', 'are', 'python', 'with', 'python', '.', 'all', 'python', 'have', 'python', 'poorli', 'at', 'least', 'onc', '.']\n"
     ]
    }
   ],
   "source": [
    "new_text = \"It is important to by very pythonly while you are pythoning with python. \\\n",
    "All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "stem_words = [ps.stem(w) for w in word_tokenize(new_text)]\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging with NLTK Ê†áËØÜÂåñÂ§ÑÁêÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer, sent_tokenize\n",
    "#nltk.download('state_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(PunktSentenceTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31005\n",
      "33411\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text))\n",
    "print(len(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "tokenized[4] # obviously the date is wrong here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_sent_tokenizer2 = sent_tokenize\n",
    "tokenized2 = custom_sent_tokenizer2(sample_text)\n",
    "tokenized2[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\",\n",
       " 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.',\n",
       " 'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.',\n",
       " '(Applause.)',\n",
       " 'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006.',\n",
       " \"White House photo by Eric DraperEvery time I'm invited to this rostrum, I'm humbled by the privilege, and mindful of the history we've seen together.\",\n",
       " 'We have gathered under this Capitol dome in moments of national mourning and national achievement.',\n",
       " 'We have served America through one of the most consequential periods of our history -- and it has been my honor to serve with you.',\n",
       " 'In a system of two parties, two chambers, and two elected branches, there will always be differences and debate.',\n",
       " 'But even tough debates can be conducted in a civil tone, and our differences cannot be allowed to harden into anger.',\n",
       " 'To confront the great issues before us, we must act in a spirit of goodwill and respect for one another -- and I will do my part.',\n",
       " 'Tonight the state of our Union is strong -- and together we will make it stronger.',\n",
       " '(Applause.)',\n",
       " 'In this decisive year, you and I will make choices that determine both the future and the character of our country.',\n",
       " 'We will choose to act confidently in pursuing the enemies of freedom -- or retreat from our duties in the hope of an easier life.',\n",
       " 'We will choose to build our prosperity by leading the world economy -- or shut ourselves off from trade and opportunity.',\n",
       " 'In a complex and challenging time, the road of isolationism and protectionism may seem broad and inviting -- yet it ends in danger and decline.',\n",
       " 'The only way to protect our people, the only way to secure the peace, the only way to control our destiny is by our leadership -- so the United States of America will continue to lead.',\n",
       " '(Applause.)',\n",
       " 'Abroad, our nation is committed to an historic, long-term goal -- we seek the end of tyranny in our world.',\n",
       " 'Some dismiss that goal as misguided idealism.',\n",
       " 'In reality, the future security of America depends on it.',\n",
       " 'On September the 11th, 2001, we found that problems originating in a failed and oppressive state 7,000 miles away could bring murder and destruction to our country.',\n",
       " 'Dictatorships shelter terrorists, and feed resentment and radicalism, and seek weapons of mass destruction.',\n",
       " 'Democracies replace resentment with hope, respect the rights of their citizens and their neighbors, and join the fight against terror.',\n",
       " \"Every step toward freedom in the world makes our country safer -- so we will act boldly in freedom's cause.\",\n",
       " '(Applause.)',\n",
       " 'Far from being a hopeless dream, the advance of freedom is the great story of our time.',\n",
       " 'In 1945, there were about two dozen lonely democracies in the world.',\n",
       " 'Today, there are 122.',\n",
       " \"And we're writing a new chapter in the story of self-government -- with women lining up to vote in Afghanistan, and millions of Iraqis marking their liberty with purple ink, and men and women from Lebanon to Egypt debating the rights of individuals and the necessity of freedom.\",\n",
       " 'At the start of 2006, more than half the people of our world live in democratic nations.',\n",
       " 'And we do not forget the other half -- in places like Syria and Burma, Zimbabwe, North Korea, and Iran -- because the demands of justice, and the peace of this world, require their freedom, as well.',\n",
       " '(Applause.)',\n",
       " 'President George W. Bush delivers his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006.',\n",
       " 'White House photo by Eric Draper No one can deny the success of freedom, but some men rage and fight against it.',\n",
       " 'And one of the main sources of reaction and opposition is radical Islam -- the perversion by a few of a noble faith into an ideology of terror and death.',\n",
       " 'Terrorists like bin Laden are serious about mass murder -- and all of us must take their declared intentions seriously.',\n",
       " 'They seek to impose a heartless system of totalitarian control throughout the Middle East, and arm themselves with weapons of mass murder.',\n",
       " 'Their aim is to seize power in Iraq, and use it as a safe haven to launch attacks against America and the world.',\n",
       " 'Lacking the military strength to challenge us directly, the terrorists have chosen the weapon of fear.',\n",
       " 'When they murder children at a school in Beslan, or blow up commuters in London, or behead a bound captive, the terrorists hope these horrors will break our will, allowing the violent to inherit the Earth.',\n",
       " 'But they have miscalculated: We love our freedom, and we will fight to keep it.',\n",
       " '(Applause.)',\n",
       " 'In a time of testing, we cannot find security by abandoning our commitments and retreating within our borders.',\n",
       " 'If we were to leave these vicious attackers alone, they would not leave us alone.',\n",
       " 'They would simply move the battlefield to our own shores.',\n",
       " 'There is no peace in retreat.',\n",
       " 'And there is no honor in retreat.',\n",
       " 'By allowing radical Islam to work its will -- by leaving an assaulted world to fend for itself -- we would signal to all that we no longer believe in our own ideals, or even in our own courage.',\n",
       " 'But our enemies and our friends can be certain: The United States will not retreat from the world, and we will never surrender to evil.',\n",
       " '(Applause.)',\n",
       " 'America rejects the false comfort of isolationism.',\n",
       " 'We are the nation that saved liberty in Europe, and liberated death camps, and helped raise up democracies, and faced down an evil empire.',\n",
       " 'Once again, we accept the call of history to deliver the oppressed and move this world toward peace.',\n",
       " 'We remain on the offensive against terror networks.',\n",
       " 'We have killed or captured many of their leaders -- and for the others, their day will come.',\n",
       " 'President George W. Bush greets members of Congress after his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006.',\n",
       " 'White House photo by Eric Draper We remain on the offensive in Afghanistan, where a fine President and a National Assembly are fighting terror while building the institutions of a new democracy.',\n",
       " \"We're on the offensive in Iraq, with a clear plan for victory.\",\n",
       " \"First, we're helping Iraqis build an inclusive government, so that old resentments will be eased and the insurgency will be marginalized.\",\n",
       " \"Second, we're continuing reconstruction efforts, and helping the Iraqi government to fight corruption and build a modern economy, so all Iraqis can experience the benefits of freedom.\",\n",
       " \"And, third, we're striking terrorist targets while we train Iraqi forces that are increasingly capable of defeating the enemy.\",\n",
       " 'Iraqis are showing their courage every day, and we are proud to be their allies in the cause of freedom.',\n",
       " '(Applause.)',\n",
       " 'Our work in Iraq is difficult because our enemy is brutal.',\n",
       " 'But that brutality has not stopped the dramatic progress of a new democracy.',\n",
       " 'In less than three years, the nation has gone from dictatorship to liberation, to sovereignty, to a constitution, to national elections.',\n",
       " 'At the same time, our coalition has been relentless in shutting off terrorist infiltration, clearing out insurgent strongholds, and turning over territory to Iraqi security forces.',\n",
       " 'I am confident in our plan for victory; I am confident in the will of the Iraqi people; I am confident in the skill and spirit of our military.',\n",
       " 'Fellow citizens, we are in this fight to win, and we are winning.',\n",
       " '(Applause.)',\n",
       " 'The road of victory is the road that will take our troops home.',\n",
       " 'As we make progress on the ground, and Iraqi forces increasingly take the lead, we should be able to further decrease our troop levels -- but those decisions will be made by our military commanders, not by politicians in Washington, D.C.',\n",
       " '(Applause.)',\n",
       " 'Our coalition has learned from our experience in Iraq.',\n",
       " \"We've adjusted our military tactics and changed our approach to reconstruction.\",\n",
       " 'Along the way, we have benefitted from responsible criticism and counsel offered by members of Congress of both parties.',\n",
       " 'In the coming year, I will continue to reach out and seek your good advice.',\n",
       " 'Yet, there is a difference between responsible criticism that aims for success, and defeatism that refuses to acknowledge anything but failure.',\n",
       " '(Applause.)',\n",
       " 'Hindsight alone is not wisdom, and second-guessing is not a strategy.',\n",
       " '(Applause.)',\n",
       " 'With so much in the balance, those of us in public office have a duty to speak with candor.',\n",
       " 'A sudden withdrawal of our forces from Iraq would abandon our Iraqi allies to death and prison, would put men like bin Laden and Zarqawi in charge of a strategic country, and show that a pledge from America means little.',\n",
       " 'Members of Congress, however we feel about the decisions and debates of the past, our nation has only one option: We must keep our word, defeat our enemies, and stand behind the American military in this vital mission.',\n",
       " '(Applause.)',\n",
       " 'Laura Bush is applauded as she is introduced Tuesday evening, Jan. 31, 2006 during the State of the Union Address at United States Capitol in Washington.',\n",
       " 'White House photo by Eric Draper Our men and women in uniform are making sacrifices -- and showing a sense of duty stronger than all fear.',\n",
       " \"They know what it's like to fight house to house in a maze of streets, to wear heavy gear in the desert heat, to see a comrade killed by a roadside bomb.\",\n",
       " 'And those who know the costs also know the stakes.',\n",
       " 'Marine Staff Sergeant Dan Clay was killed last month fighting in Fallujah.',\n",
       " 'He left behind a letter to his family, but his words could just as well be addressed to every American.',\n",
       " 'Here is what Dan wrote: \"I know what honor is.',\n",
       " '...',\n",
       " 'It has been an honor to protect and serve all of you.',\n",
       " 'I faced death with the secure knowledge that you would not have to.... Never falter!',\n",
       " 'Don\\'t hesitate to honor and support those of us who have the honor of protecting that which is worth protecting.\"',\n",
       " \"Staff Sergeant Dan Clay's wife, Lisa, and his mom and dad, Sara Jo and Bud, are with us this evening.\",\n",
       " 'Welcome.',\n",
       " '(Applause.)',\n",
       " 'Our nation is grateful to the fallen, who live in the memory of our country.',\n",
       " \"We're grateful to all who volunteer to wear our nation's uniform -- and as we honor our brave troops, let us never forget the sacrifices of America's military families.\",\n",
       " '(Applause.)',\n",
       " 'Our offensive against terror involves more than military action.',\n",
       " 'Ultimately, the only way to defeat the terrorists is to defeat their dark vision of hatred and fear by offering the hopeful alternative of political freedom and peaceful change.',\n",
       " 'So the United States of America supports democratic reform across the broader Middle East.',\n",
       " 'Elections are vital, but they are only the beginning.',\n",
       " 'Raising up a democracy requires the rule of law, and protection of minorities, and strong, accountable institutions that last longer than a single vote.',\n",
       " 'The great people of Egypt have voted in a multi-party presidential election -- and now their government should open paths of peaceful opposition that will reduce the appeal of radicalism.',\n",
       " 'The Palestinian people have voted in elections.',\n",
       " 'And now the leaders of Hamas must recognize Israel, disarm, reject terrorism, and work for lasting peace.',\n",
       " '(Applause.)',\n",
       " 'Saudi Arabia has taken the first steps of reform -- now it can offer its people a better future by pressing forward with those efforts.',\n",
       " 'Democracies in the Middle East will not look like our own, because they will reflect the traditions of their own citizens.',\n",
       " 'Yet liberty is the future of every nation in the Middle East, because liberty is the right and hope of all humanity.',\n",
       " '(Applause.)',\n",
       " 'President George W. Bush waves toward the upper visitors gallery of the House Chamber following his State of the Union remarks Tuesday, Jan. 31, 2006 at the United States Capitol.',\n",
       " 'White House photo by Eric Draper The same is true of Iran, a nation now held hostage by a small clerical elite that is isolating and repressing its people.',\n",
       " 'The regime in that country sponsors terrorists in the Palestinian territories and in Lebanon -- and that must come to an end.',\n",
       " '(Applause.)',\n",
       " 'The Iranian government is defying the world with its nuclear ambitions, and the nations of the world must not permit the Iranian regime to gain nuclear weapons.',\n",
       " '(Applause.)',\n",
       " 'America will continue to rally the world to confront these threats.',\n",
       " 'Tonight, let me speak directly to the citizens of Iran: America respects you, and we respect your country.',\n",
       " 'We respect your right to choose your own future and win your own freedom.',\n",
       " 'And our nation hopes one day to be the closest of friends with a free and democratic Iran.',\n",
       " '(Applause.)',\n",
       " 'To overcome dangers in our world, we must also take the offensive by encouraging economic progress, and fighting disease, and spreading hope in hopeless lands.',\n",
       " 'Isolationism would not only tie our hands in fighting enemies, it would keep us from helping our friends in desperate need.',\n",
       " 'We show compassion abroad because Americans believe in the God-given dignity and worth of a villager with HIV/AIDS, or an infant with malaria, or a refugee fleeing genocide, or a young girl sold into slavery.',\n",
       " 'We also show compassion abroad because regions overwhelmed by poverty, corruption, and despair are sources of terrorism, and organized crime, and human trafficking, and the drug trade.',\n",
       " 'In recent years, you and I have taken unprecedented action to fight AIDS and malaria, expand the education of girls, and reward developing nations that are moving forward with economic and political reform.',\n",
       " 'For people everywhere, the United States is a partner for a better life.',\n",
       " 'Short-changing these efforts would increase the suffering and chaos of our world, undercut our long-term security, and dull the conscience of our country.',\n",
       " 'I urge members of Congress to serve the interests of America by showing the compassion of America.',\n",
       " 'Our country must also remain on the offensive against terrorism here at home.',\n",
       " 'The enemy has not lost the desire or capability to attack us.',\n",
       " 'Fortunately, this nation has superb professionals in law enforcement, intelligence, the military, and homeland security.',\n",
       " 'These men and women are dedicating their lives, protecting us all, and they deserve our support and our thanks.',\n",
       " '(Applause.)',\n",
       " 'They also deserve the same tools they already use to fight drug trafficking and organized crime -- so I ask you to reauthorize the Patriot Act.',\n",
       " '(Applause.)',\n",
       " 'It is said that prior to the attacks of September the 11th, our government failed to connect the dots of the conspiracy.',\n",
       " 'We now know that two of the hijackers in the United States placed telephone calls to al Qaeda operatives overseas.',\n",
       " 'But we did not know about their plans until it was too late.',\n",
       " 'So to prevent another attack -- based on authority given to me by the Constitution and by statute -- I have authorized a terrorist surveillance program to aggressively pursue the international communications of suspected al Qaeda operatives and affiliates to and from America.',\n",
       " 'Previous Presidents have used the same constitutional authority I have, and federal courts have approved the use of that authority.',\n",
       " 'Appropriate members of Congress have been kept informed.',\n",
       " 'The terrorist surveillance program has helped prevent terrorist attacks.',\n",
       " 'It remains essential to the security of America.',\n",
       " 'If there are people inside our country who are talking with al Qaeda, we want to know about it, because we will not sit back and wait to be hit again.',\n",
       " '(Applause.)',\n",
       " 'In all these areas -- from the disruption of terror networks, to victory in Iraq, to the spread of freedom and hope in troubled regions -- we need the support of our friends and allies.',\n",
       " 'To draw that support, we must always be clear in our principles and willing to act.',\n",
       " 'The only alternative to American leadership is a dramatically more dangerous and anxious world.',\n",
       " 'Yet we also choose to lead because it is a privilege to serve the values that gave us birth.',\n",
       " 'American leaders -- from Roosevelt to Truman to Kennedy to Reagan -- rejected isolation and retreat, because they knew that America is always more secure when freedom is on the march.',\n",
       " 'Our own generation is in a long war against a determined enemy -- a war that will be fought by Presidents of both parties, who will need steady bipartisan support from the Congress.',\n",
       " 'And tonight I ask for yours.',\n",
       " 'Together, let us protect our country, support the men and women who defend us, and lead this world toward freedom.',\n",
       " '(Applause.)',\n",
       " 'Here at home, America also has a great opportunity: We will build the prosperity of our country by strengthening our economic leadership in the world.',\n",
       " 'Our economy is healthy and vigorous, and growing faster than other major industrialized nations.',\n",
       " 'In the last two-and-a-half years, America has created 4.6 million new jobs -- more than Japan and the European Union combined.',\n",
       " '(Applause.)',\n",
       " 'Even in the face of higher energy prices and natural disasters, the American people have turned in an economic performance that is the envy of the world.',\n",
       " 'The American economy is preeminent, but we cannot afford to be complacent.',\n",
       " \"In a dynamic world economy, we are seeing new competitors, like China and India, and this creates uncertainty, which makes it easier to feed people's fears.\",\n",
       " \"So we're seeing some old temptations return.\",\n",
       " 'Protectionists want to escape competition, pretending that we can keep our high standard of living while walling off our economy.',\n",
       " 'Others say that the government needs to take a larger role in directing the economy, centralizing more power in Washington and increasing taxes.',\n",
       " 'We hear claims that immigrants are somehow bad for the economy -- even though this economy could not function without them.',\n",
       " '(Applause.)',\n",
       " 'All these are forms of economic retreat, and they lead in the same direction -- toward a stagnant and second-rate economy.',\n",
       " 'Tonight I will set out a better path: an agenda for a nation that competes with confidence; an agenda that will raise standards of living and generate new jobs.',\n",
       " 'Americans should not fear our economic future, because we intend to shape it.',\n",
       " 'Keeping America competitive begins with keeping our economy growing.',\n",
       " 'And our economy grows when Americans have more of their own money to spend, save, and invest.',\n",
       " 'In the last five years, the tax relief you passed has left $880 billion in the hands of American workers, investors, small businesses, and families -- and they have used it to help produce more than four years of uninterrupted economic growth.',\n",
       " '(Applause.)',\n",
       " 'Yet the tax relief is set to expire in the next few years.',\n",
       " 'If we do nothing, American families will face a massive tax increase they do not expect and will not welcome.',\n",
       " 'Because America needs more than a temporary expansion, we need more than temporary tax relief.',\n",
       " 'I urge the Congress to act responsibly, and make the tax cuts permanent.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires us to be good stewards of tax dollars.',\n",
       " \"Every year of my presidency, we've reduced the growth of non-security discretionary spending, and last year you passed bills that cut this spending.\",\n",
       " 'This year my budget will cut it again, and reduce or eliminate more than 140 programs that are performing poorly or not fulfilling essential priorities.',\n",
       " 'By passing these reforms, we will save the American taxpayer another $14 billion next year, and stay on track to cut the deficit in half by 2009.',\n",
       " '(Applause.)',\n",
       " 'I am pleased that members of Congress are working on earmark reform, because the federal budget has too many special interest projects.',\n",
       " '(Applause.)',\n",
       " 'And we can tackle this problem together, if you pass the line-item veto.',\n",
       " '(Applause.)',\n",
       " 'We must also confront the larger challenge of mandatory spending, or entitlements.',\n",
       " \"This year, the first of about 78 million baby boomers turn 60, including two of my Dad's favorite people -- me and President Clinton.\",\n",
       " '(Laughter.)',\n",
       " 'This milestone is more than a personal crisis -- (laughter) -- it is a national challenge.',\n",
       " 'The retirement of the baby boom generation will put unprecedented strains on the federal government.',\n",
       " 'By 2030, spending for Social Security, Medicare and Medicaid alone will be almost 60 percent of the entire federal budget.',\n",
       " 'And that will present future Congresses with impossible choices -- staggering tax increases, immense deficits, or deep cuts in every category of spending.',\n",
       " 'Congress did not act last year on my proposal to save Social Security -- (applause) -- yet the rising cost of entitlements is a problem that is not going away.',\n",
       " '(Applause.)',\n",
       " 'And every year we fail to act, the situation gets worse.',\n",
       " 'So tonight, I ask you to join me in creating a commission to examine the full impact of baby boom retirements on Social Security, Medicare, and Medicaid.',\n",
       " 'This commission should include members of Congress of both parties, and offer bipartisan solutions.',\n",
       " 'We need to put aside partisan politics and work together and get this problem solved.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires us to open more markets for all that Americans make and grow.',\n",
       " 'One out of every five factory jobs in America is related to global trade, and we want people everywhere to buy American.',\n",
       " 'With open markets and a level playing field, no one can out-produce or out-compete the American worker.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires an immigration system that upholds our laws, reflects our values, and serves the interests of our economy.',\n",
       " 'Our nation needs orderly and secure borders.',\n",
       " '(Applause.)',\n",
       " 'To meet this goal, we must have stronger immigration enforcement and border protection.',\n",
       " '(Applause.)',\n",
       " 'And we must have a rational, humane guest worker program that rejects amnesty, allows temporary jobs for people who seek them legally, and reduces smuggling and crime at the border.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires affordable health care.',\n",
       " '(Applause.)',\n",
       " 'Our government has a responsibility to provide health care for the poor and the elderly, and we are meeting that responsibility.',\n",
       " '(Applause.)',\n",
       " 'For all Americans -- for all Americans, we must confront the rising cost of care, strengthen the doctor-patient relationship, and help people afford the insurance coverage they need.',\n",
       " '(Applause.)',\n",
       " 'We will make wider use of electronic records and other health information technology, to help control costs and reduce dangerous medical errors.',\n",
       " 'We will strengthen health savings accounts -- making sure individuals and small business employees can buy insurance with the same advantages that people working for big businesses now get.',\n",
       " '(Applause.)',\n",
       " 'We will do more to make this coverage portable, so workers can switch jobs without having to worry about losing their health insurance.',\n",
       " '(Applause.)',\n",
       " 'And because lawsuits are driving many good doctors out of practice -- leaving women in nearly 1,500 American counties without a single OB/GYN -- I ask the Congress to pass medical liability reform this year.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires affordable energy.',\n",
       " 'And here we have a serious problem: America is addicted to oil, which is often imported from unstable parts of the world.',\n",
       " 'The best way to break this addiction is through technology.',\n",
       " 'Since 2001, we have spent nearly $10 billion to develop cleaner, cheaper, and more reliable alternative energy sources -- and we are on the threshold of incredible advances.',\n",
       " 'So tonight, I announce the Advanced Energy Initiative -- a 22-percent increase in clean-energy research -- at the Department of Energy, to push for breakthroughs in two vital areas.',\n",
       " 'To change how we power our homes and offices, we will invest more in zero-emission coal-fired plants, revolutionary solar and wind technologies, and clean, safe nuclear energy.',\n",
       " '(Applause.)',\n",
       " 'We must also change how we power our automobiles.',\n",
       " 'We will increase our research in better batteries for hybrid and electric cars, and in pollution-free cars that run on hydrogen.',\n",
       " \"We'll also fund additional research in cutting-edge methods of producing ethanol, not just from corn, but from wood chips and stalks, or switch grass.\",\n",
       " 'Our goal is to make this new kind of ethanol practical and competitive within six years.',\n",
       " '(Applause.)',\n",
       " 'Breakthroughs on this and other new technologies will help us reach another great goal: to replace more than 75 percent of our oil imports from the Middle East by 2025.',\n",
       " '(Applause.)',\n",
       " 'By applying the talent and technology of America, this country can dramatically improve our environment, move beyond a petroleum-based economy, and make our dependence on Middle Eastern oil a thing of the past.',\n",
       " '(Applause.)',\n",
       " 'And to keep America competitive, one commitment is necessary above all: We must continue to lead the world in human talent and creativity.',\n",
       " \"Our greatest advantage in the world has always been our educated, hardworking, ambitious people -- and we're going to keep that edge.\",\n",
       " \"Tonight I announce an American Competitiveness Initiative, to encourage innovation throughout our economy, and to give our nation's children a firm grounding in math and science.\",\n",
       " '(Applause.)',\n",
       " 'First, I propose to double the federal commitment to the most critical basic research programs in the physical sciences over the next 10 years.',\n",
       " \"This funding will support the work of America's most creative minds as they explore promising areas such as nanotechnology, supercomputing, and alternative energy sources.\",\n",
       " 'Second, I propose to make permanent the research and development tax credit -- (applause) -- to encourage bolder private-sector initiatives in technology.',\n",
       " 'With more research in both the public and private sectors, we will improve our quality of life -- and ensure that America will lead the world in opportunity and innovation for decades to come.',\n",
       " '(Applause.)',\n",
       " 'Third, we need to encourage children to take more math and science, and to make sure those courses are rigorous enough to compete with other nations.',\n",
       " \"We've made a good start in the early grades with the No Child Left Behind Act, which is raising standards and lifting test scores across our country.\",\n",
       " 'Tonight I propose to train 70,000 high school teachers to lead advanced-placement courses in math and science, bring 30,000 math and science professionals to teach in classrooms, and give early help to students who struggle with math, so they have a better chance at good, high-wage jobs.',\n",
       " \"If we ensure that America's children succeed in life, they will ensure that America succeeds in the world.\",\n",
       " '(Applause.)',\n",
       " 'Preparing our nation to compete in the world is a goal that all of us can share.',\n",
       " 'I urge you to support the American Competitiveness Initiative, and together we will show the world what the American people can achieve.',\n",
       " 'America is a great force for freedom and prosperity.',\n",
       " 'Yet our greatness is not measured in power or luxuries, but by who we are and how we treat one another.',\n",
       " 'So we strive to be a compassionate, decent, hopeful society.',\n",
       " 'In recent years, America has become a more hopeful nation.',\n",
       " 'Violent crime rates have fallen to their lowest levels since the 1970s.',\n",
       " 'Welfare cases have dropped by more than half over the past decade.',\n",
       " 'Drug use among youth is down 19 percent since 2001.',\n",
       " 'There are fewer abortions in America than at any point in the last three decades, and the number of children born to teenage mothers has been falling for a dozen years in a row.',\n",
       " '(Applause.)',\n",
       " 'These gains are evidence of a quiet transformation -- a revolution of conscience, in which a rising generation is finding that a life of personal responsibility is a life of fulfillment.',\n",
       " 'Government has played a role.',\n",
       " 'Wise policies, such as welfare reform and drug education and support for abstinence and adoption have made a difference in the character of our country.',\n",
       " 'And everyone here tonight, Democrat and Republican, has a right to be proud of this record.',\n",
       " '(Applause.)',\n",
       " 'Yet many Americans, especially parents, still have deep concerns about the direction of our culture, and the health of our most basic institutions.',\n",
       " \"They're concerned about unethical conduct by public officials, and discouraged by activist courts that try to redefine marriage.\",\n",
       " 'They worry about children in our society who need direction and love, and about fellow citizens still displaced by natural disaster, and about suffering caused by treatable diseases.',\n",
       " 'As we look at these challenges, we must never give in to the belief that America is in decline, or that our culture is doomed to unravel.',\n",
       " 'The American people know better than that.',\n",
       " 'We have proven the pessimists wrong before -- and we will do it again.',\n",
       " '(Applause.)',\n",
       " 'A hopeful society depends on courts that deliver equal justice under the law.',\n",
       " 'The Supreme Court now has two superb new members -- new members on its bench: Chief Justice John Roberts and Justice Sam Alito.',\n",
       " '(Applause.)',\n",
       " 'I thank the Senate for confirming both of them.',\n",
       " 'I will continue to nominate men and women who understand that judges must be servants of the law, and not legislate from the bench.',\n",
       " '(Applause.)',\n",
       " 'Today marks the official retirement of a very special American.',\n",
       " \"For 24 years of faithful service to our nation, the United States is grateful to Justice Sandra Day O'Connor.\",\n",
       " '(Applause.)',\n",
       " 'A hopeful society has institutions of science and medicine that do not cut ethical corners, and that recognize the matchless value of every life.',\n",
       " 'Tonight I ask you to pass legislation to prohibit the most egregious abuses of medical research: human cloning in all its forms, creating or implanting embryos for experiments, creating human-animal hybrids, and buying, selling, or patenting human embryos.',\n",
       " 'Human life is a gift from our Creator -- and that gift should never be discarded, devalued or put up for sale.',\n",
       " '(Applause.)',\n",
       " 'A hopeful society expects elected officials to uphold the public trust.',\n",
       " '(Applause.)',\n",
       " 'Honorable people in both parties are working on reforms to strengthen the ethical standards of Washington -- I support your efforts.',\n",
       " 'Each of us has made a pledge to be worthy of public responsibility -- and that is a pledge we must never forget, never dismiss, and never betray.',\n",
       " '(Applause.)',\n",
       " 'As we renew the promise of our institutions, let us also show the character of America in our compassion and care for one another.',\n",
       " 'A hopeful society gives special attention to children who lack direction and love.',\n",
       " \"Through the Helping America's Youth Initiative, we are encouraging caring adults to get involved in the life of a child -- and this good work is being led by our First Lady, Laura Bush.\",\n",
       " '(Applause.)',\n",
       " \"This year we will add resources to encourage young people to stay in school, so more of America's youth can raise their sights and achieve their dreams.\",\n",
       " \"A hopeful society comes to the aid of fellow citizens in times of suffering and emergency -- and stays at it until they're back on their feet.\",\n",
       " 'So far the federal government has committed $85 billion to the people of the Gulf Coast and New Orleans.',\n",
       " \"We're removing debris and repairing highways and rebuilding stronger levees.\",\n",
       " \"We're providing business loans and housing assistance.\",\n",
       " 'Yet as we meet these immediate needs, we must also address deeper challenges that existed before the storm arrived.',\n",
       " 'In New Orleans and in other places, many of our fellow citizens have felt excluded from the promise of our country.',\n",
       " 'The answer is not only temporary relief, but schools that teach every child, and job skills that bring upward mobility, and more opportunities to own a home and start a business.',\n",
       " 'As we recover from a disaster, let us also work for the day when all Americans are protected by justice, equal in hope, and rich in opportunity.',\n",
       " '(Applause.)',\n",
       " 'A hopeful society acts boldly to fight diseases like HIV/AIDS, which can be prevented, and treated, and defeated.',\n",
       " 'More than a million Americans live with HIV, and half of all AIDS cases occur among African Americans.',\n",
       " 'I ask Congress to reform and reauthorize the Ryan White Act, and provide new funding to states, so we end the waiting lists for AIDS medicines in America.',\n",
       " '(Applause.)',\n",
       " 'We will also lead a nationwide effort, working closely with African American churches and faith-based groups, to deliver rapid HIV tests to millions, end the stigma of AIDS, and come closer to the day when there are no new infections in America.',\n",
       " '(Applause.)',\n",
       " \"Fellow citizens, we've been called to leadership in a period of consequence.\",\n",
       " \"We've entered a great ideological conflict we did nothing to invite.\",\n",
       " 'We see great changes in science and commerce that will influence all our lives.',\n",
       " 'Sometimes it can seem that history is turning in a wide arc, toward an unknown shore.',\n",
       " 'Yet the destination of history is determined by human action, and every great movement of history comes to a point of choosing.',\n",
       " 'Lincoln could have accepted peace at the cost of disunity and continued slavery.',\n",
       " 'Martin Luther King could have stopped at Birmingham or at Selma, and achieved only half a victory over segregation.',\n",
       " 'The United States could have accepted the permanent division of Europe, and been complicit in the oppression of others.',\n",
       " 'Today, having come far in our own historical journey, we must decide: Will we turn back, or finish well?',\n",
       " 'Before history is written down in books, it is written in courage.',\n",
       " 'Like Americans before us, we will show that courage and we will finish well.',\n",
       " \"We will lead freedom's advance.\",\n",
       " 'We will compete and excel in the global economy.',\n",
       " 'We will renew the defining moral commitments of this land.',\n",
       " 'And so we move forward -- optimistic about our country, faithful to its cause, and confident of the victories to come.',\n",
       " 'May God bless America.',\n",
       " '(Applause.)']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[w[0] == w[1] for w in zip(tokenized, tokenized2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(tokenized):\n",
    "    try:\n",
    "        tagged = [nltk.pos_tag(nltk.word_tokenize(w)) \\\n",
    "                  for w in tokenized]\n",
    "        return tagged\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_tagging = process_content(tokenized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRESIDENT', 'NNP'),\n",
       " ('GEORGE', 'NNP'),\n",
       " ('W.', 'NNP'),\n",
       " ('BUSH', 'NNP'),\n",
       " (\"'S\", 'POS'),\n",
       " ('ADDRESS', 'NNP'),\n",
       " ('BEFORE', 'IN'),\n",
       " ('A', 'NNP'),\n",
       " ('JOINT', 'NNP'),\n",
       " ('SESSION', 'NNP'),\n",
       " ('OF', 'IN'),\n",
       " ('THE', 'NNP'),\n",
       " ('CONGRESS', 'NNP'),\n",
       " ('ON', 'NNP'),\n",
       " ('THE', 'NNP'),\n",
       " ('STATE', 'NNP'),\n",
       " ('OF', 'IN'),\n",
       " ('THE', 'NNP'),\n",
       " ('UNION', 'NNP'),\n",
       " ('January', 'NNP'),\n",
       " ('31', 'CD'),\n",
       " (',', ','),\n",
       " ('2006', 'CD'),\n",
       " ('THE', 'NNP'),\n",
       " ('PRESIDENT', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_tagging[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`Chunking`](https://pythonprogramming.net/chunking-nltk-tutorial/?completed=/part-of-speech-tagging-nltk-tutorial/)\n",
    "Âà©Áî®chunkÊâægroup of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "'''\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent\\'s\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "\n",
    "Regular expressions:\n",
    "\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or MORE repetitions\t  \n",
    ". = Any character except a new line\n",
    "'''\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            yield chunked   \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "chuncked_list = list(process_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chuncked_list[1].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr.', 'NNP', 'B-Chunk'),\n",
      " ('Speaker', 'NNP', 'I-Chunk'),\n",
      " (',', ',', 'O'),\n",
      " ('Vice', 'NNP', 'B-Chunk'),\n",
      " ('President', 'NNP', 'I-Chunk'),\n",
      " ('Cheney', 'NNP', 'I-Chunk'),\n",
      " (',', ',', 'O'),\n",
      " ('members', 'NNS', 'O'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('Congress', 'NNP', 'B-Chunk'),\n",
      " (',', ',', 'O'),\n",
      " ('members', 'NNS', 'O'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('Supreme', 'NNP', 'B-Chunk'),\n",
      " ('Court', 'NNP', 'I-Chunk'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('diplomatic', 'JJ', 'O'),\n",
      " ('corps', 'NN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('distinguished', 'JJ', 'O'),\n",
      " ('guests', 'NNS', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('fellow', 'JJ', 'O'),\n",
      " ('citizens', 'NNS', 'O'),\n",
      " (':', ':', 'O'),\n",
      " ('Today', 'VB', 'O'),\n",
      " ('our', 'PRP$', 'O'),\n",
      " ('nation', 'NN', 'O'),\n",
      " ('lost', 'VBD', 'O'),\n",
      " ('a', 'DT', 'O'),\n",
      " ('beloved', 'VBN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('graceful', 'JJ', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('courageous', 'JJ', 'O'),\n",
      " ('woman', 'NN', 'O'),\n",
      " ('who', 'WP', 'O'),\n",
      " ('called', 'VBD', 'B-Chunk'),\n",
      " ('America', 'NNP', 'I-Chunk'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('founding', 'NN', 'O'),\n",
      " ('ideals', 'NNS', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('carried', 'VBD', 'O'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('a', 'DT', 'O'),\n",
      " ('noble', 'JJ', 'O'),\n",
      " ('dream', 'NN', 'O'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "iob_tagged = tree2conlltags(chuncked_list[1])\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chinking](https://pythonprogramming.net/chinking-nltk-tutorial/?completed=/chunking-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Named Entity Recognition with NLTK](https://pythonprogramming.net/named-entity-recognition-nltk-tutorial/?completed=/chinking-nltk-tutorial/): error rate is high\n",
    "```python\n",
    "NE Type and Examples\n",
    "ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "PERSON - Eddy Bonte, President Obama\n",
    "LOCATION - Murray River, Mount Everest\n",
    "DATE - June, 2008-06-29\n",
    "TIME - two fifty a m, 1:30 p.m.\n",
    "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT - twenty pct, 18.75 %\n",
    "FACILITY - Washington Monument, Stonehenge\n",
    "GPE - South East Asia, Midlothian\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content(tokenized):\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            yield namedEnt\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "namedEnt = list(process_content(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__radd__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_frozen_class',\n",
       " '_get_node',\n",
       " '_label',\n",
       " '_parse_error',\n",
       " '_pformat_flat',\n",
       " '_repr_png_',\n",
       " '_set_node',\n",
       " 'append',\n",
       " 'chomsky_normal_form',\n",
       " 'clear',\n",
       " 'collapse_unary',\n",
       " 'convert',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'draw',\n",
       " 'extend',\n",
       " 'flatten',\n",
       " 'freeze',\n",
       " 'fromstring',\n",
       " 'height',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'label',\n",
       " 'leaf_treeposition',\n",
       " 'leaves',\n",
       " 'node',\n",
       " 'pformat',\n",
       " 'pformat_latex_qtree',\n",
       " 'pop',\n",
       " 'pos',\n",
       " 'pprint',\n",
       " 'pretty_print',\n",
       " 'productions',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'set_label',\n",
       " 'sort',\n",
       " 'subtrees',\n",
       " 'treeposition_spanning_leaves',\n",
       " 'treepositions',\n",
       " 'un_chomsky_normal_form',\n",
       " 'unicode_repr']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(namedEnt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tree in module nltk.tree object:\n",
      "\n",
      "class Tree(builtins.list)\n",
      " |  A Tree represents a hierarchical grouping of leaves and subtrees.\n",
      " |  For example, each constituent in a syntax tree is represented by a single Tree.\n",
      " |  \n",
      " |  A tree's children are encoded as a list of leaves and subtrees,\n",
      " |  where a leaf is a basic (non-tree) value; and a subtree is a\n",
      " |  nested Tree.\n",
      " |  \n",
      " |      >>> from nltk.tree import Tree\n",
      " |      >>> print(Tree(1, [2, Tree(3, [4]), 5]))\n",
      " |      (1 2 (3 4) 5)\n",
      " |      >>> vp = Tree('VP', [Tree('V', ['saw']),\n",
      " |      ...                  Tree('NP', ['him'])])\n",
      " |      >>> s = Tree('S', [Tree('NP', ['I']), vp])\n",
      " |      >>> print(s)\n",
      " |      (S (NP I) (VP (V saw) (NP him)))\n",
      " |      >>> print(s[1])\n",
      " |      (VP (V saw) (NP him))\n",
      " |      >>> print(s[1,1])\n",
      " |      (NP him)\n",
      " |      >>> t = Tree.fromstring(\"(S (NP I) (VP (V saw) (NP him)))\")\n",
      " |      >>> s == t\n",
      " |      True\n",
      " |      >>> t[1][1].set_label('X')\n",
      " |      >>> t[1][1].label()\n",
      " |      'X'\n",
      " |      >>> print(t)\n",
      " |      (S (NP I) (VP (V saw) (X him)))\n",
      " |      >>> t[0], t[1,1] = t[1,1], t[0]\n",
      " |      >>> print(t)\n",
      " |      (S (X him) (VP (V saw) (NP I)))\n",
      " |  \n",
      " |  The length of a tree is the number of children it has.\n",
      " |  \n",
      " |      >>> len(t)\n",
      " |      2\n",
      " |  \n",
      " |  The set_label() and label() methods allow individual constituents\n",
      " |  to be labeled.  For example, syntax trees use this label to specify\n",
      " |  phrase tags, such as \"NP\" and \"VP\".\n",
      " |  \n",
      " |  Several Tree methods use \"tree positions\" to specify\n",
      " |  children or descendants of a tree.  Tree positions are defined as\n",
      " |  follows:\n",
      " |  \n",
      " |    - The tree position *i* specifies a Tree's *i*\\ th child.\n",
      " |    - The tree position ``()`` specifies the Tree itself.\n",
      " |    - If *p* is the tree position of descendant *d*, then\n",
      " |      *p+i* specifies the *i*\\ th child of *d*.\n",
      " |  \n",
      " |  I.e., every tree position is either a single index *i*,\n",
      " |  specifying ``tree[i]``; or a sequence *i1, i2, ..., iN*,\n",
      " |  specifying ``tree[i1][i2]...[iN]``.\n",
      " |  \n",
      " |  Construct a new tree.  This constructor can be called in one\n",
      " |  of two ways:\n",
      " |  \n",
      " |  - ``Tree(label, children)`` constructs a new tree with the\n",
      " |      specified label and list of children.\n",
      " |  \n",
      " |  - ``Tree.fromstring(s)`` constructs a new tree by parsing the string ``s``.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Tree\n",
      " |      builtins.list\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, v)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __delitem__(self, index)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__ lambda self, other\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__ lambda self, other\n",
      " |  \n",
      " |  __init__(self, node, children=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __le__ lambda self, other\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, v)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__ lambda self, other\n",
      " |      # @total_ordering doesn't work here, since the class inherits from a builtin class\n",
      " |  \n",
      " |  __radd__(self, v)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmul__(self, v)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  __setitem__(self, index, value)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self)\n",
      " |  \n",
      " |  chomsky_normal_form(self, factor='right', horzMarkov=None, vertMarkov=0, childChar='|', parentChar='^')\n",
      " |      This method can modify a tree in three ways:\n",
      " |      \n",
      " |        1. Convert a tree into its Chomsky Normal Form (CNF)\n",
      " |           equivalent -- Every subtree has either two non-terminals\n",
      " |           or one terminal as its children.  This process requires\n",
      " |           the creation of more\"artificial\" non-terminal nodes.\n",
      " |        2. Markov (vertical) smoothing of children in new artificial\n",
      " |           nodes\n",
      " |        3. Horizontal (parent) annotation of nodes\n",
      " |      \n",
      " |      :param factor: Right or left factoring method (default = \"right\")\n",
      " |      :type  factor: str = [left|right]\n",
      " |      :param horzMarkov: Markov order for sibling smoothing in artificial nodes (None (default) = include all siblings)\n",
      " |      :type  horzMarkov: int | None\n",
      " |      :param vertMarkov: Markov order for parent smoothing (0 (default) = no vertical annotation)\n",
      " |      :type  vertMarkov: int | None\n",
      " |      :param childChar: A string used in construction of the artificial nodes, separating the head of the\n",
      " |                        original subtree from the child nodes that have yet to be expanded (default = \"|\")\n",
      " |      :type  childChar: str\n",
      " |      :param parentChar: A string used to separate the node representation from its vertical annotation\n",
      " |      :type  parentChar: str\n",
      " |  \n",
      " |  collapse_unary(self, collapsePOS=False, collapseRoot=False, joinChar='+')\n",
      " |      Collapse subtrees with a single child (ie. unary productions)\n",
      " |      into a new non-terminal (Tree node) joined by 'joinChar'.\n",
      " |      This is useful when working with algorithms that do not allow\n",
      " |      unary productions, and completely removing the unary productions\n",
      " |      would require loss of useful information.  The Tree is modified\n",
      " |      directly (since it is passed by reference) and no value is returned.\n",
      " |      \n",
      " |      :param collapsePOS: 'False' (default) will not collapse the parent of leaf nodes (ie.\n",
      " |                          Part-of-Speech tags) since they are always unary productions\n",
      " |      :type  collapsePOS: bool\n",
      " |      :param collapseRoot: 'False' (default) will not modify the root production\n",
      " |                           if it is unary.  For the Penn WSJ treebank corpus, this corresponds\n",
      " |                           to the TOP -> productions.\n",
      " |      :type collapseRoot: bool\n",
      " |      :param joinChar: A string used to connect collapsed node values (default = \"+\")\n",
      " |      :type  joinChar: str\n",
      " |  \n",
      " |  copy(self, deep=False)\n",
      " |      L.copy() -> list -- a shallow copy of L\n",
      " |  \n",
      " |  draw(self)\n",
      " |      Open a new window containing a graphical diagram of this tree.\n",
      " |  \n",
      " |  flatten(self)\n",
      " |      Return a flat version of the tree, with all non-root non-terminals removed.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> print(t.flatten())\n",
      " |          (S the dog chased the cat)\n",
      " |      \n",
      " |      :return: a tree consisting of this tree's root connected directly to\n",
      " |          its leaves, omitting all intervening non-terminal nodes.\n",
      " |      :rtype: Tree\n",
      " |  \n",
      " |  freeze(self, leaf_freezer=None)\n",
      " |  \n",
      " |  height(self)\n",
      " |      Return the height of the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.height()\n",
      " |          5\n",
      " |          >>> print(t[0,0])\n",
      " |          (D the)\n",
      " |          >>> t[0,0].height()\n",
      " |          2\n",
      " |      \n",
      " |      :return: The height of this tree.  The height of a tree\n",
      " |          containing no children is 1; the height of a tree\n",
      " |          containing only leaves is 2; and the height of any other\n",
      " |          tree is one plus the maximum of its children's\n",
      " |          heights.\n",
      " |      :rtype: int\n",
      " |  \n",
      " |  label(self)\n",
      " |      Return the node label of the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')\n",
      " |          >>> t.label()\n",
      " |          'S'\n",
      " |      \n",
      " |      :return: the node label (typically a string)\n",
      " |      :rtype: any\n",
      " |  \n",
      " |  leaf_treeposition(self, index)\n",
      " |      :return: The tree position of the ``index``-th leaf in this\n",
      " |          tree.  I.e., if ``tp=self.leaf_treeposition(i)``, then\n",
      " |          ``self[tp]==self.leaves()[i]``.\n",
      " |      \n",
      " |      :raise IndexError: If this tree contains fewer than ``index+1``\n",
      " |          leaves, or if ``index<0``.\n",
      " |  \n",
      " |  leaves(self)\n",
      " |      Return the leaves of the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.leaves()\n",
      " |          ['the', 'dog', 'chased', 'the', 'cat']\n",
      " |      \n",
      " |      :return: a list containing this tree's leaves.\n",
      " |          The order reflects the order of the\n",
      " |          leaves in the tree's hierarchical structure.\n",
      " |      :rtype: list\n",
      " |  \n",
      " |  pformat(self, margin=70, indent=0, nodesep='', parens='()', quotes=False)\n",
      " |      :return: A pretty-printed string representation of this tree.\n",
      " |      :rtype: str\n",
      " |      :param margin: The right margin at which to do line-wrapping.\n",
      " |      :type margin: int\n",
      " |      :param indent: The indentation level at which printing\n",
      " |          begins.  This number is used to decide how far to indent\n",
      " |          subsequent lines.\n",
      " |      :type indent: int\n",
      " |      :param nodesep: A string that is used to separate the node\n",
      " |          from the children.  E.g., the default value ``':'`` gives\n",
      " |          trees like ``(S: (NP: I) (VP: (V: saw) (NP: it)))``.\n",
      " |  \n",
      " |  pformat_latex_qtree(self)\n",
      " |      Returns a representation of the tree compatible with the\n",
      " |      LaTeX qtree package. This consists of the string ``\\Tree``\n",
      " |      followed by the tree represented in bracketed notation.\n",
      " |      \n",
      " |      For example, the following result was generated from a parse tree of\n",
      " |      the sentence ``The announcement astounded us``::\n",
      " |      \n",
      " |        \\Tree [.I'' [.N'' [.D The ] [.N' [.N announcement ] ] ]\n",
      " |            [.I' [.V'' [.V' [.V astounded ] [.N'' [.N' [.N us ] ] ] ] ] ] ]\n",
      " |      \n",
      " |      See http://www.ling.upenn.edu/advice/latex.html for the LaTeX\n",
      " |      style file for the qtree package.\n",
      " |      \n",
      " |      :return: A latex qtree representation of this tree.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  pos(self)\n",
      " |      Return a sequence of pos-tagged words extracted from the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.pos()\n",
      " |          [('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]\n",
      " |      \n",
      " |      :return: a list of tuples containing leaves and pre-terminals (part-of-speech tags).\n",
      " |          The order reflects the order of the leaves in the tree's hierarchical structure.\n",
      " |      :rtype: list(tuple)\n",
      " |  \n",
      " |  pprint(self, **kwargs)\n",
      " |      Print a string representation of this Tree to 'stream'\n",
      " |  \n",
      " |  pretty_print(self, sentence=None, highlight=(), stream=None, **kwargs)\n",
      " |      Pretty-print this tree as ASCII or Unicode art.\n",
      " |      For explanation of the arguments, see the documentation for\n",
      " |      `nltk.treeprettyprinter.TreePrettyPrinter`.\n",
      " |  \n",
      " |  productions(self)\n",
      " |      Generate the productions that correspond to the non-terminal nodes of the tree.\n",
      " |      For each subtree of the form (P: C1 C2 ... Cn) this produces a production of the\n",
      " |      form P -> C1 C2 ... Cn.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.productions()\n",
      " |          [S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',\n",
      " |          NP -> D N, D -> 'the', N -> 'cat']\n",
      " |      \n",
      " |      :rtype: list(Production)\n",
      " |  \n",
      " |  set_label(self, label)\n",
      " |      Set the node label of the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.set_label(\"T\")\n",
      " |          >>> print(t)\n",
      " |          (T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\n",
      " |      \n",
      " |      :param label: the node label (typically a string)\n",
      " |      :type label: any\n",
      " |  \n",
      " |  subtrees(self, filter=None)\n",
      " |      Generate all the subtrees of this tree, optionally restricted\n",
      " |      to trees matching the filter function.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> for s in t.subtrees(lambda t: t.height() == 2):\n",
      " |          ...     print(s)\n",
      " |          (D the)\n",
      " |          (N dog)\n",
      " |          (V chased)\n",
      " |          (D the)\n",
      " |          (N cat)\n",
      " |      \n",
      " |      :type filter: function\n",
      " |      :param filter: the function to filter all local trees\n",
      " |  \n",
      " |  treeposition_spanning_leaves(self, start, end)\n",
      " |      :return: The tree position of the lowest descendant of this\n",
      " |          tree that dominates ``self.leaves()[start:end]``.\n",
      " |      :raise ValueError: if ``end <= start``\n",
      " |  \n",
      " |  treepositions(self, order='preorder')\n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.treepositions() # doctest: +ELLIPSIS\n",
      " |          [(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]\n",
      " |          >>> for pos in t.treepositions('leaves'):\n",
      " |          ...     t[pos] = t[pos][::-1].upper()\n",
      " |          >>> print(t)\n",
      " |          (S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))\n",
      " |      \n",
      " |      :param order: One of: ``preorder``, ``postorder``, ``bothorder``,\n",
      " |          ``leaves``.\n",
      " |  \n",
      " |  un_chomsky_normal_form(self, expandUnary=True, childChar='|', parentChar='^', unaryChar='+')\n",
      " |      This method modifies the tree in three ways:\n",
      " |      \n",
      " |        1. Transforms a tree in Chomsky Normal Form back to its\n",
      " |           original structure (branching greater than two)\n",
      " |        2. Removes any parent annotation (if it exists)\n",
      " |        3. (optional) expands unary subtrees (if previously\n",
      " |           collapsed with collapseUnary(...) )\n",
      " |      \n",
      " |      :param expandUnary: Flag to expand unary or not (default = True)\n",
      " |      :type  expandUnary: bool\n",
      " |      :param childChar: A string separating the head node from its children in an artificial node (default = \"|\")\n",
      " |      :type  childChar: str\n",
      " |      :param parentChar: A sting separating the node label from its parent annotation (default = \"^\")\n",
      " |      :type  parentChar: str\n",
      " |      :param unaryChar: A string joining two non-terminals in a unary production (default = \"+\")\n",
      " |      :type  unaryChar: str\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  convert(tree) from builtins.type\n",
      " |      Convert a tree between different subtypes of Tree.  ``cls`` determines\n",
      " |      which class will be used to encode the new tree.\n",
      " |      \n",
      " |      :type tree: Tree\n",
      " |      :param tree: The tree that should be converted.\n",
      " |      :return: The new Tree.\n",
      " |  \n",
      " |  fromstring(s, brackets='()', read_node=None, read_leaf=None, node_pattern=None, leaf_pattern=None, remove_empty_top_bracketing=False) from builtins.type\n",
      " |      Read a bracketed tree string and return the resulting tree.\n",
      " |      Trees are represented as nested brackettings, such as::\n",
      " |      \n",
      " |        (S (NP (NNP John)) (VP (V runs)))\n",
      " |      \n",
      " |      :type s: str\n",
      " |      :param s: The string to read\n",
      " |      \n",
      " |      :type brackets: str (length=2)\n",
      " |      :param brackets: The bracket characters used to mark the\n",
      " |          beginning and end of trees and subtrees.\n",
      " |      \n",
      " |      :type read_node: function\n",
      " |      :type read_leaf: function\n",
      " |      :param read_node, read_leaf: If specified, these functions\n",
      " |          are applied to the substrings of ``s`` corresponding to\n",
      " |          nodes and leaves (respectively) to obtain the values for\n",
      " |          those nodes and leaves.  They should have the following\n",
      " |          signature:\n",
      " |      \n",
      " |             read_node(str) -> value\n",
      " |      \n",
      " |          For example, these functions could be used to process nodes\n",
      " |          and leaves whose values should be some type other than\n",
      " |          string (such as ``FeatStruct``).\n",
      " |          Note that by default, node strings and leaf strings are\n",
      " |          delimited by whitespace and brackets; to override this\n",
      " |          default, use the ``node_pattern`` and ``leaf_pattern``\n",
      " |          arguments.\n",
      " |      \n",
      " |      :type node_pattern: str\n",
      " |      :type leaf_pattern: str\n",
      " |      :param node_pattern, leaf_pattern: Regular expression patterns\n",
      " |          used to find node and leaf substrings in ``s``.  By\n",
      " |          default, both nodes patterns are defined to match any\n",
      " |          sequence of non-whitespace non-bracket characters.\n",
      " |      \n",
      " |      :type remove_empty_top_bracketing: bool\n",
      " |      :param remove_empty_top_bracketing: If the resulting tree has\n",
      " |          an empty node label, and is length one, then return its\n",
      " |          single child instead.  This is useful for treebank trees,\n",
      " |          which sometimes contain an extra level of bracketing.\n",
      " |      \n",
      " |      :return: A tree corresponding to the string representation ``s``.\n",
      " |          If this class method is called using a subclass of Tree,\n",
      " |          then it will return a tree of that type.\n",
      " |      :rtype: Tree\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  node\n",
      " |      Outdated method to access the node value; use the label() method instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.list:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iadd__(self, value, /)\n",
      " |      Implement self+=value.\n",
      " |  \n",
      " |  __imul__(self, value, /)\n",
      " |      Implement self*=value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __reversed__(...)\n",
      " |      L.__reversed__() -- return a reverse iterator over the list\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      L.__sizeof__() -- size of L in memory, in bytes\n",
      " |  \n",
      " |  append(...)\n",
      " |      L.append(object) -> None -- append object to end\n",
      " |  \n",
      " |  clear(...)\n",
      " |      L.clear() -> None -- remove all items from L\n",
      " |  \n",
      " |  count(...)\n",
      " |      L.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  extend(...)\n",
      " |      L.extend(iterable) -> None -- extend list by appending elements from the iterable\n",
      " |  \n",
      " |  index(...)\n",
      " |      L.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  insert(...)\n",
      " |      L.insert(index, object) -- insert object before index\n",
      " |  \n",
      " |  pop(...)\n",
      " |      L.pop([index]) -> item -- remove and return item at index (default last).\n",
      " |      Raises IndexError if list is empty or index is out of range.\n",
      " |  \n",
      " |  remove(...)\n",
      " |      L.remove(value) -> None -- remove first occurrence of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  reverse(...)\n",
      " |      L.reverse() -- reverse *IN PLACE*\n",
      " |  \n",
      " |  sort(...)\n",
      " |      L.sort(key=None, reverse=False) -> None -- stable sort *IN PLACE*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(namedEnt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEnt[3].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Lemmatizing with NLTK](https://pythonprogramming.net/lemmatizing-nltk-tutorial/?completed=/named-entity-recognition-nltk-tutorial/): better than stemming, ËØçÊÄßËøòÂéü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The corpora with NLTK](https://pythonprogramming.net/nltk-corpus-corpora-tutorial/?completed=/lemmatizing-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\n",
      "\n",
      "The Natural Language Toolkit (NLTK) is an open source Python library\n",
      "for Natural Language Processing.  A free online book is available.\n",
      "(If you use the library for academic research, please cite the book.)\n",
      "\n",
      "Steven Bird, Ewan Klein, and Edward Loper (2009).\n",
      "Natural Language Processing with Python.  O'Reilly Media Inc.\n",
      "http://nltk.org/book\n",
      "\n",
      "@version: 3.3\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__file__)\n",
    "print(nltk.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LazyModule in nltk.nltk object:\n",
      "\n",
      "nltk.nltk.corpus = class LazyModule(builtins.object)\n",
      " |  Lazy module class.\n",
      " |  \n",
      " |  Lazy modules are imported into the given namespaces whenever a\n",
      " |  non-special attribute (there are some attributes like __doc__\n",
      " |  that class instances handle without calling __getattr__) is\n",
      " |  requested. The module is then registered under the given name\n",
      " |  in locals usually replacing the import wrapper instance. The\n",
      " |  import itself is done using globals as global namespace.\n",
      " |  \n",
      " |  Example of creating a lazy load module:\n",
      " |  \n",
      " |  ISO = LazyModule('ISO',locals(),globals())\n",
      " |  \n",
      " |  Later, requesting an attribute from ISO will load the module\n",
      " |  automatically into the locals() namespace, overriding the\n",
      " |  LazyModule instance:\n",
      " |  \n",
      " |  t = ISO.Week(1998,1,1)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Import the module on demand and get the attribute.\n",
      " |  \n",
      " |  __init__(self, name, locals, globals=None)\n",
      " |      Create a LazyModule instance wrapping module name.\n",
      " |      \n",
      " |      The module will later on be registered in locals under the\n",
      " |      given module name.\n",
      " |      \n",
      " |      globals is optional and defaults to locals.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Import the module on demand and set the attribute.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep.\n",
      "And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# sample text\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample)\n",
    "\n",
    "for x in range(5):\n",
    "    print(tok[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Wordnet with NLTK](https://pythonprogramming.net/wordnet-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Text Classification with NLTK](https://pythonprogramming.net/text-classification-nltk-tutorial/?completed=/wordnet-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(movie_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = movie_reviews.categories()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_id = movie_reviews.fileids('pos')\n",
    "#pos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_id = movie_reviews.fileids('neg')\n",
    "#neg_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' '.join(documents[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cat = [cat[1] for cat in documents]\n",
    "#print(list_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cat = [cat[1] for cat in documents]\n",
    "#print(list_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583820"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = [w.lower() for w in movie_reviews.words()]\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 77717, 'the': 76529, '.': 65876, 'a': 38106, 'and': 35576, 'of': 34123, 'to': 31937, \"'\": 30585, 'is': 25195, 'in': 21822, ...})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_freq = nltk.FreqDist(all_words)\n",
    "all_words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 77717),\n",
       " ('the', 76529),\n",
       " ('.', 65876),\n",
       " ('a', 38106),\n",
       " ('and', 35576),\n",
       " ('of', 34123),\n",
       " ('to', 31937),\n",
       " (\"'\", 30585),\n",
       " ('is', 25195),\n",
       " ('in', 21822),\n",
       " ('s', 18513),\n",
       " ('\"', 17612),\n",
       " ('it', 16107),\n",
       " ('that', 15924),\n",
       " ('-', 15595),\n",
       " (')', 11781),\n",
       " ('(', 11664),\n",
       " ('as', 11378),\n",
       " ('with', 10792),\n",
       " ('for', 9961),\n",
       " ('his', 9587),\n",
       " ('this', 9578),\n",
       " ('film', 9517),\n",
       " ('i', 8889),\n",
       " ('he', 8864),\n",
       " ('but', 8634),\n",
       " ('on', 7385),\n",
       " ('are', 6949),\n",
       " ('t', 6410),\n",
       " ('by', 6261),\n",
       " ('be', 6174),\n",
       " ('one', 5852),\n",
       " ('movie', 5771),\n",
       " ('an', 5744),\n",
       " ('who', 5692),\n",
       " ('not', 5577),\n",
       " ('you', 5316),\n",
       " ('from', 4999),\n",
       " ('at', 4986),\n",
       " ('was', 4940),\n",
       " ('have', 4901),\n",
       " ('they', 4825),\n",
       " ('has', 4719),\n",
       " ('her', 4522),\n",
       " ('all', 4373),\n",
       " ('?', 3771),\n",
       " ('there', 3770),\n",
       " ('like', 3690),\n",
       " ('so', 3683),\n",
       " ('out', 3637),\n",
       " ('about', 3523),\n",
       " ('up', 3405),\n",
       " ('more', 3347),\n",
       " ('what', 3322),\n",
       " ('when', 3258),\n",
       " ('which', 3161),\n",
       " ('or', 3148),\n",
       " ('she', 3141),\n",
       " ('their', 3122),\n",
       " (':', 3042),\n",
       " ('some', 2985),\n",
       " ('just', 2905),\n",
       " ('can', 2882),\n",
       " ('if', 2799),\n",
       " ('we', 2775),\n",
       " ('him', 2633),\n",
       " ('into', 2623),\n",
       " ('even', 2565),\n",
       " ('only', 2495),\n",
       " ('than', 2474),\n",
       " ('no', 2472),\n",
       " ('good', 2411),\n",
       " ('time', 2411),\n",
       " ('most', 2306),\n",
       " ('its', 2270),\n",
       " ('will', 2216),\n",
       " ('story', 2169),\n",
       " ('would', 2109),\n",
       " ('been', 2050),\n",
       " ('much', 2049),\n",
       " ('character', 2020),\n",
       " ('also', 1967),\n",
       " ('get', 1949),\n",
       " ('other', 1948),\n",
       " ('do', 1915),\n",
       " ('two', 1911),\n",
       " ('well', 1906),\n",
       " ('them', 1877),\n",
       " ('very', 1863),\n",
       " ('characters', 1859),\n",
       " (';', 1850),\n",
       " ('first', 1836),\n",
       " ('--', 1815),\n",
       " ('after', 1762),\n",
       " ('see', 1749),\n",
       " ('!', 1713),\n",
       " ('way', 1693),\n",
       " ('because', 1684),\n",
       " ('make', 1642),\n",
       " ('life', 1586),\n",
       " ('off', 1581),\n",
       " ('too', 1577),\n",
       " ('any', 1574),\n",
       " ('does', 1568),\n",
       " ('really', 1558),\n",
       " ('had', 1546),\n",
       " ('while', 1539),\n",
       " ('films', 1536),\n",
       " ('how', 1517),\n",
       " ('plot', 1513),\n",
       " ('little', 1501),\n",
       " ('where', 1492),\n",
       " ('people', 1455),\n",
       " ('over', 1446),\n",
       " ('could', 1427),\n",
       " ('then', 1424),\n",
       " ('me', 1414),\n",
       " ('scene', 1397),\n",
       " ('man', 1396),\n",
       " ('bad', 1395),\n",
       " ('my', 1394),\n",
       " ('never', 1374),\n",
       " ('being', 1338),\n",
       " ('best', 1333),\n",
       " ('these', 1309),\n",
       " ('don', 1294),\n",
       " ('new', 1292),\n",
       " ('doesn', 1277),\n",
       " ('scenes', 1274),\n",
       " ('many', 1268),\n",
       " ('director', 1237),\n",
       " ('such', 1219),\n",
       " ('know', 1217),\n",
       " ('were', 1217),\n",
       " ('movies', 1206),\n",
       " ('through', 1190),\n",
       " ('here', 1188),\n",
       " ('action', 1172),\n",
       " ('great', 1148),\n",
       " ('re', 1132),\n",
       " ('another', 1121),\n",
       " ('love', 1119),\n",
       " ('go', 1113),\n",
       " ('made', 1084),\n",
       " ('us', 1073),\n",
       " ('big', 1064),\n",
       " ('end', 1062),\n",
       " ('something', 1061),\n",
       " ('back', 1060),\n",
       " ('*', 1054),\n",
       " ('still', 1047),\n",
       " ('world', 1037),\n",
       " ('seems', 1033),\n",
       " ('work', 1020),\n",
       " ('those', 1015),\n",
       " ('makes', 992),\n",
       " ('now', 992),\n",
       " ('before', 992),\n",
       " ('however', 989),\n",
       " ('between', 965),\n",
       " ('few', 964),\n",
       " ('/', 960),\n",
       " ('down', 950),\n",
       " ('every', 947),\n",
       " ('though', 940),\n",
       " ('better', 922),\n",
       " ('real', 915),\n",
       " ('audience', 914),\n",
       " ('enough', 910),\n",
       " ('seen', 910),\n",
       " ('take', 905),\n",
       " ('around', 903),\n",
       " ('both', 893),\n",
       " ('going', 888),\n",
       " ('year', 886),\n",
       " ('performance', 886),\n",
       " ('why', 884),\n",
       " ('should', 872),\n",
       " ('role', 872),\n",
       " ('isn', 871),\n",
       " ('same', 870),\n",
       " ('old', 870),\n",
       " ('gets', 865),\n",
       " ('your', 858),\n",
       " ('may', 857),\n",
       " ('things', 852),\n",
       " ('think', 849),\n",
       " ('years', 846),\n",
       " ('last', 843),\n",
       " ('comedy', 840),\n",
       " ('funny', 840),\n",
       " ('actually', 837),\n",
       " ('ve', 836),\n",
       " ('long', 836),\n",
       " ('look', 835),\n",
       " ('almost', 820),\n",
       " ('own', 815),\n",
       " ('thing', 809),\n",
       " ('fact', 805),\n",
       " ('nothing', 804),\n",
       " ('say', 802),\n",
       " ('right', 798),\n",
       " ('john', 798),\n",
       " ('although', 795),\n",
       " ('played', 791),\n",
       " ('find', 782),\n",
       " ('script', 778),\n",
       " ('come', 777),\n",
       " ('ever', 776),\n",
       " ('cast', 769),\n",
       " ('since', 768),\n",
       " ('did', 763),\n",
       " ('star', 761),\n",
       " ('plays', 752),\n",
       " ('young', 743),\n",
       " ('show', 741),\n",
       " ('comes', 733),\n",
       " ('m', 727),\n",
       " ('part', 714),\n",
       " ('original', 712),\n",
       " ('actors', 706),\n",
       " ('screen', 705),\n",
       " ('without', 697),\n",
       " ('again', 696),\n",
       " ('acting', 695),\n",
       " ('three', 695),\n",
       " ('day', 688),\n",
       " ('each', 687),\n",
       " ('point', 685),\n",
       " ('lot', 677),\n",
       " ('least', 676),\n",
       " ('takes', 674),\n",
       " ('guy', 664),\n",
       " ('quite', 660),\n",
       " ('himself', 656),\n",
       " ('away', 655),\n",
       " ('during', 655),\n",
       " ('family', 652),\n",
       " ('effects', 649),\n",
       " ('course', 648),\n",
       " ('goes', 646),\n",
       " ('minutes', 644),\n",
       " ('interesting', 638),\n",
       " ('might', 635),\n",
       " ('far', 635),\n",
       " ('high', 624),\n",
       " ('rather', 621),\n",
       " ('once', 621),\n",
       " ('must', 618),\n",
       " ('anything', 618),\n",
       " ('place', 615),\n",
       " ('set', 610),\n",
       " ('yet', 608),\n",
       " ('watch', 603),\n",
       " ('d', 603),\n",
       " ('making', 602),\n",
       " ('our', 601),\n",
       " ('wife', 588),\n",
       " ('hard', 587),\n",
       " ('always', 586),\n",
       " ('fun', 584),\n",
       " ('didn', 579),\n",
       " ('ll', 577),\n",
       " ('seem', 574),\n",
       " ('special', 574),\n",
       " ('bit', 568),\n",
       " ('times', 568),\n",
       " ('trying', 566),\n",
       " ('hollywood', 566),\n",
       " ('instead', 565),\n",
       " ('give', 561),\n",
       " ('want', 560),\n",
       " ('picture', 560),\n",
       " ('kind', 559),\n",
       " ('american', 559),\n",
       " ('job', 556),\n",
       " ('sense', 555),\n",
       " ('woman', 554),\n",
       " ('home', 550),\n",
       " ('having', 549),\n",
       " ('series', 548),\n",
       " ('actor', 546),\n",
       " ('probably', 539),\n",
       " ('help', 537),\n",
       " ('half', 535),\n",
       " ('along', 533),\n",
       " ('men', 532),\n",
       " ('everything', 530),\n",
       " ('pretty', 528),\n",
       " ('becomes', 526),\n",
       " ('sure', 523),\n",
       " ('black', 523),\n",
       " ('together', 522),\n",
       " ('dialogue', 519),\n",
       " ('money', 512),\n",
       " ('become', 511),\n",
       " ('gives', 507),\n",
       " ('given', 502),\n",
       " ('looking', 501),\n",
       " ('whole', 495),\n",
       " ('watching', 493),\n",
       " ('father', 492),\n",
       " ('`', 486),\n",
       " ('feel', 485),\n",
       " ('everyone', 484),\n",
       " ('music', 480),\n",
       " ('wants', 477),\n",
       " ('sex', 476),\n",
       " ('less', 475),\n",
       " ('done', 475),\n",
       " ('horror', 473),\n",
       " ('got', 470),\n",
       " ('death', 468),\n",
       " ('perhaps', 464),\n",
       " ('city', 459),\n",
       " ('next', 458),\n",
       " ('especially', 456),\n",
       " ('play', 453),\n",
       " ('girl', 452),\n",
       " ('mind', 451),\n",
       " ('10', 449),\n",
       " ('moments', 447),\n",
       " ('looks', 443),\n",
       " ('completely', 440),\n",
       " ('2', 439),\n",
       " ('reason', 437),\n",
       " ('mother', 437),\n",
       " ('whose', 436),\n",
       " ('line', 435),\n",
       " ('night', 435),\n",
       " ('human', 432),\n",
       " ('until', 431),\n",
       " ('rest', 431),\n",
       " ('performances', 431),\n",
       " ('different', 430),\n",
       " ('evil', 429),\n",
       " ('small', 429),\n",
       " ('james', 429),\n",
       " ('simply', 428),\n",
       " ('couple', 427),\n",
       " ('put', 426),\n",
       " ('let', 425),\n",
       " ('anyone', 424),\n",
       " ('ending', 423),\n",
       " ('case', 420),\n",
       " ('several', 419),\n",
       " ('dead', 418),\n",
       " ('michael', 417),\n",
       " ('left', 413),\n",
       " ('thought', 413),\n",
       " ('school', 411),\n",
       " ('shows', 410),\n",
       " ('humor', 410),\n",
       " ('true', 410),\n",
       " ('lost', 409),\n",
       " ('written', 409),\n",
       " ('itself', 409),\n",
       " ('friend', 409),\n",
       " ('entire', 408),\n",
       " ('getting', 406),\n",
       " ('town', 404),\n",
       " ('turns', 403),\n",
       " ('soon', 402),\n",
       " ('someone', 401),\n",
       " ('second', 401),\n",
       " ('main', 399),\n",
       " ('stars', 399),\n",
       " ('found', 398),\n",
       " ('use', 398),\n",
       " ('problem', 396),\n",
       " ('friends', 396),\n",
       " ('tv', 395),\n",
       " ('top', 393),\n",
       " ('name', 392),\n",
       " ('begins', 392),\n",
       " ('called', 391),\n",
       " ('based', 389),\n",
       " ('comic', 389),\n",
       " ('david', 388),\n",
       " ('head', 387),\n",
       " ('else', 387),\n",
       " ('idea', 386),\n",
       " ('either', 386),\n",
       " ('wrong', 385),\n",
       " ('unfortunately', 382),\n",
       " ('later', 382),\n",
       " ('final', 380),\n",
       " ('hand', 378),\n",
       " ('alien', 378),\n",
       " ('house', 377),\n",
       " ('group', 377),\n",
       " ('full', 375),\n",
       " ('used', 372),\n",
       " ('tries', 370),\n",
       " ('often', 370),\n",
       " ('against', 370),\n",
       " ('war', 369),\n",
       " ('sequence', 366),\n",
       " ('keep', 364),\n",
       " ('turn', 363),\n",
       " ('playing', 362),\n",
       " ('boy', 362),\n",
       " ('behind', 362),\n",
       " ('named', 362),\n",
       " ('certainly', 361),\n",
       " ('live', 361),\n",
       " ('believe', 360),\n",
       " ('under', 359),\n",
       " ('works', 359),\n",
       " ('relationship', 359),\n",
       " ('face', 357),\n",
       " ('hour', 355),\n",
       " ('run', 354),\n",
       " ('style', 354),\n",
       " ('said', 353),\n",
       " ('despite', 352),\n",
       " ('person', 352),\n",
       " ('finally', 350),\n",
       " ('shot', 348),\n",
       " ('book', 347),\n",
       " ('doing', 347),\n",
       " ('tell', 346),\n",
       " ('maybe', 345),\n",
       " ('nice', 344),\n",
       " ('son', 343),\n",
       " ('perfect', 343),\n",
       " ('side', 341),\n",
       " ('seeing', 339),\n",
       " ('able', 339),\n",
       " ('finds', 337),\n",
       " ('children', 336),\n",
       " ('days', 336),\n",
       " ('past', 336),\n",
       " ('summer', 334),\n",
       " ('camera', 333),\n",
       " ('won', 332),\n",
       " ('including', 330),\n",
       " ('mr', 329),\n",
       " ('kids', 328),\n",
       " ('lives', 327),\n",
       " ('directed', 326),\n",
       " ('moment', 325),\n",
       " ('game', 325),\n",
       " ('running', 323),\n",
       " ('fight', 323),\n",
       " ('supposed', 323),\n",
       " ('video', 322),\n",
       " ('car', 321),\n",
       " ('matter', 321),\n",
       " ('kevin', 321),\n",
       " ('joe', 321),\n",
       " ('lines', 320),\n",
       " ('worth', 319),\n",
       " ('=', 319),\n",
       " ('daughter', 317),\n",
       " ('earth', 317),\n",
       " ('starts', 316),\n",
       " ('need', 316),\n",
       " ('entertaining', 314),\n",
       " ('white', 314),\n",
       " ('start', 312),\n",
       " ('writer', 312),\n",
       " ('dark', 312),\n",
       " ('short', 310),\n",
       " ('self', 309),\n",
       " ('worst', 308),\n",
       " ('nearly', 307),\n",
       " ('opening', 306),\n",
       " ('try', 306),\n",
       " ('upon', 306),\n",
       " ('care', 304),\n",
       " ('early', 303),\n",
       " ('violence', 303),\n",
       " ('throughout', 302),\n",
       " ('team', 302),\n",
       " ('production', 300),\n",
       " ('example', 300),\n",
       " ('beautiful', 299),\n",
       " ('title', 297),\n",
       " ('exactly', 297),\n",
       " ('jack', 297),\n",
       " ('review', 295),\n",
       " ('major', 295),\n",
       " ('drama', 294),\n",
       " ('&', 293),\n",
       " ('problems', 293),\n",
       " ('sequences', 293),\n",
       " ('obvious', 292),\n",
       " ('version', 292),\n",
       " ('screenplay', 292),\n",
       " ('known', 292),\n",
       " ('killer', 291),\n",
       " ('wasn', 291),\n",
       " ('robert', 291),\n",
       " ('disney', 290),\n",
       " ('already', 289),\n",
       " ('close', 289),\n",
       " ('classic', 289),\n",
       " ('others', 288),\n",
       " ('hit', 285),\n",
       " ('kill', 285),\n",
       " ('deep', 285),\n",
       " ('five', 284),\n",
       " ('order', 284),\n",
       " ('act', 284),\n",
       " ('simple', 284),\n",
       " ('fine', 284),\n",
       " ('themselves', 283),\n",
       " ('heart', 283),\n",
       " ('roles', 283),\n",
       " ('jackie', 283),\n",
       " ('direction', 283),\n",
       " ('eyes', 282),\n",
       " ('four', 282),\n",
       " ('question', 278),\n",
       " ('sort', 278),\n",
       " ('sometimes', 277),\n",
       " ('knows', 277),\n",
       " ('supporting', 276),\n",
       " ('coming', 275),\n",
       " ('voice', 275),\n",
       " ('women', 275),\n",
       " ('truly', 274),\n",
       " ('save', 273),\n",
       " ('jokes', 273),\n",
       " ('computer', 273),\n",
       " ('child', 272),\n",
       " ('o', 271),\n",
       " ('boring', 270),\n",
       " ('tom', 270),\n",
       " ('level', 270),\n",
       " ('1', 270),\n",
       " ('body', 269),\n",
       " ('guys', 268),\n",
       " ('genre', 268),\n",
       " ('brother', 268),\n",
       " ('strong', 268),\n",
       " ('stop', 268),\n",
       " ('room', 268),\n",
       " ('space', 267),\n",
       " ('lee', 266),\n",
       " ('ends', 266),\n",
       " ('beginning', 265),\n",
       " ('ship', 264),\n",
       " ('york', 264),\n",
       " ('attempt', 263),\n",
       " ('thriller', 263),\n",
       " ('scream', 262),\n",
       " ('peter', 261),\n",
       " ('aren', 260),\n",
       " ('husband', 260),\n",
       " ('fiction', 258),\n",
       " ('happens', 258),\n",
       " ('hero', 258),\n",
       " ('novel', 257),\n",
       " ('note', 257),\n",
       " ('hope', 257),\n",
       " ('king', 256),\n",
       " ('yes', 256),\n",
       " ('says', 256),\n",
       " ('tells', 255),\n",
       " ('quickly', 255),\n",
       " ('romantic', 254),\n",
       " ('dog', 254),\n",
       " ('oscar', 254),\n",
       " ('stupid', 253),\n",
       " ('possible', 253),\n",
       " ('saw', 253),\n",
       " ('lead', 251),\n",
       " ('career', 251),\n",
       " ('murder', 250),\n",
       " ('extremely', 248),\n",
       " ('manages', 248),\n",
       " ('god', 248),\n",
       " ('mostly', 247),\n",
       " ('wonder', 247),\n",
       " ('particularly', 246),\n",
       " ('future', 246),\n",
       " ('fans', 245),\n",
       " ('sound', 245),\n",
       " ('worse', 245),\n",
       " ('piece', 244),\n",
       " ('involving', 244),\n",
       " ('de', 244),\n",
       " ('appears', 243),\n",
       " ('planet', 243),\n",
       " ('paul', 243),\n",
       " ('involved', 243),\n",
       " ('mean', 242),\n",
       " ('none', 242),\n",
       " ('taking', 242),\n",
       " ('hours', 242),\n",
       " ('laugh', 242),\n",
       " ('police', 241),\n",
       " ('sets', 241),\n",
       " ('attention', 241),\n",
       " ('co', 241),\n",
       " ('hell', 240),\n",
       " ('eventually', 240),\n",
       " ('single', 239),\n",
       " ('fall', 239),\n",
       " ('falls', 239),\n",
       " ('material', 239),\n",
       " ('emotional', 239),\n",
       " ('power', 238),\n",
       " ('late', 238),\n",
       " ('lack', 237),\n",
       " ('dr', 237),\n",
       " ('van', 237),\n",
       " ('result', 237),\n",
       " ('elements', 236),\n",
       " ('meet', 236),\n",
       " ('smith', 236),\n",
       " ('science', 235),\n",
       " ('experience', 235),\n",
       " ('bring', 235),\n",
       " ('wild', 235),\n",
       " ('living', 234),\n",
       " ('theater', 234),\n",
       " ('interest', 232),\n",
       " ('leads', 231),\n",
       " ('word', 231),\n",
       " ('feature', 230),\n",
       " ('battle', 230),\n",
       " ('girls', 229),\n",
       " ('alone', 229),\n",
       " ('obviously', 228),\n",
       " ('george', 228),\n",
       " ('within', 227),\n",
       " ('usually', 227),\n",
       " ('enjoy', 227),\n",
       " ('guess', 226),\n",
       " ('among', 226),\n",
       " ('taken', 225),\n",
       " ('feeling', 225),\n",
       " ('laughs', 225),\n",
       " ('aliens', 225),\n",
       " ('talk', 224),\n",
       " ('chance', 223),\n",
       " ('talent', 223),\n",
       " ('3', 222),\n",
       " ('middle', 222),\n",
       " ('number', 222),\n",
       " ('easy', 222),\n",
       " ('across', 221),\n",
       " ('needs', 221),\n",
       " ('attempts', 221),\n",
       " ('happen', 220),\n",
       " ('television', 220),\n",
       " ('chris', 220),\n",
       " ('deal', 219),\n",
       " ('poor', 219),\n",
       " ('form', 219),\n",
       " ('girlfriend', 218),\n",
       " ('viewer', 218),\n",
       " ('release', 218),\n",
       " ('killed', 218),\n",
       " ('forced', 218),\n",
       " ('whether', 217),\n",
       " ('wonderful', 217),\n",
       " ('feels', 216),\n",
       " ('oh', 216),\n",
       " ('tale', 216),\n",
       " ('serious', 216),\n",
       " ('expect', 216),\n",
       " ('except', 216),\n",
       " ('light', 216),\n",
       " ('success', 216),\n",
       " ('features', 216),\n",
       " ('premise', 216),\n",
       " ('happy', 215),\n",
       " ('words', 215),\n",
       " ('leave', 215),\n",
       " ('important', 215),\n",
       " ('meets', 215),\n",
       " ('history', 215),\n",
       " ('giving', 214),\n",
       " ('crew', 214),\n",
       " ('type', 214),\n",
       " ('call', 214),\n",
       " ('turned', 214),\n",
       " ('released', 214),\n",
       " ('parents', 214),\n",
       " ('art', 214),\n",
       " ('impressive', 214),\n",
       " ('mission', 213),\n",
       " ('working', 213),\n",
       " ('seemed', 212),\n",
       " ('score', 212),\n",
       " ('told', 212),\n",
       " ('recent', 211),\n",
       " ('robin', 211),\n",
       " ('basically', 211),\n",
       " ('entertainment', 211),\n",
       " ('america', 211),\n",
       " ('$', 210),\n",
       " ('surprise', 210),\n",
       " ('apparently', 209),\n",
       " ('easily', 209),\n",
       " ('ryan', 209),\n",
       " ('cool', 208),\n",
       " ('stuff', 208),\n",
       " ('cop', 208),\n",
       " ('change', 208),\n",
       " ('williams', 208),\n",
       " ('crime', 208),\n",
       " ('office', 208),\n",
       " ('parts', 207),\n",
       " ('somehow', 207),\n",
       " ('sequel', 207),\n",
       " ('william', 206),\n",
       " ('cut', 206),\n",
       " ('die', 205),\n",
       " ('jones', 204),\n",
       " ('credits', 203),\n",
       " ('batman', 203),\n",
       " ('suspense', 202),\n",
       " ('brings', 202),\n",
       " ('events', 201),\n",
       " ('reality', 201),\n",
       " ('whom', 201),\n",
       " ('local', 201),\n",
       " ('talking', 200),\n",
       " ('difficult', 200),\n",
       " ('using', 200),\n",
       " ('went', 200),\n",
       " ('writing', 200),\n",
       " ('remember', 200),\n",
       " ('near', 200),\n",
       " ('straight', 200),\n",
       " ('hilarious', 200),\n",
       " ('ago', 199),\n",
       " ('certain', 199),\n",
       " ('ben', 199),\n",
       " ('kid', 198),\n",
       " ('wouldn', 198),\n",
       " ('slow', 198),\n",
       " ('blood', 198),\n",
       " ('mystery', 198),\n",
       " ('complete', 197),\n",
       " ('red', 197),\n",
       " ('popular', 197),\n",
       " ('effective', 197),\n",
       " ('am', 197),\n",
       " ('fast', 197),\n",
       " ('flick', 196),\n",
       " ('due', 196),\n",
       " ('runs', 196),\n",
       " ('gone', 195),\n",
       " ('return', 195),\n",
       " ('presence', 195),\n",
       " ('quality', 195),\n",
       " ('dramatic', 194),\n",
       " ('filmmakers', 194),\n",
       " ('age', 194),\n",
       " ('brothers', 194),\n",
       " ('business', 193),\n",
       " ('general', 193),\n",
       " ('rock', 192),\n",
       " ('sexual', 192),\n",
       " ('present', 192),\n",
       " ('surprisingly', 191),\n",
       " ('anyway', 191),\n",
       " ('uses', 191),\n",
       " ('4', 190),\n",
       " ('personal', 190),\n",
       " ('figure', 190),\n",
       " ('smart', 190),\n",
       " ('ways', 189),\n",
       " ('decides', 189),\n",
       " ('annoying', 189),\n",
       " ('begin', 189),\n",
       " ('couldn', 189),\n",
       " ('somewhat', 188),\n",
       " ('shots', 188),\n",
       " ('rich', 188),\n",
       " ('minute', 187),\n",
       " ('law', 187),\n",
       " ('previous', 186),\n",
       " ('jim', 186),\n",
       " ('successful', 186),\n",
       " ('harry', 186),\n",
       " ('water', 186),\n",
       " ('similar', 186),\n",
       " ('absolutely', 186),\n",
       " ('motion', 186),\n",
       " ('former', 186),\n",
       " ('strange', 185),\n",
       " ('came', 185),\n",
       " ('follow', 185),\n",
       " ('read', 185),\n",
       " ('project', 185),\n",
       " ('million', 185),\n",
       " ('secret', 184),\n",
       " ('starring', 184),\n",
       " ('clear', 184),\n",
       " ('familiar', 184),\n",
       " ('romance', 184),\n",
       " ('intelligent', 184),\n",
       " ('third', 184),\n",
       " ('excellent', 184),\n",
       " ('amazing', 184),\n",
       " ('party', 183),\n",
       " ('budget', 183),\n",
       " ('eye', 183),\n",
       " ('actress', 183),\n",
       " ('prison', 183),\n",
       " ('latest', 183),\n",
       " ('means', 183),\n",
       " ('company', 183),\n",
       " ('towards', 182),\n",
       " ('predictable', 182),\n",
       " ('powerful', 182),\n",
       " ('nor', 182),\n",
       " ('bob', 182),\n",
       " ('beyond', 181),\n",
       " ('visual', 181),\n",
       " ('leaves', 181),\n",
       " ('r', 180),\n",
       " ('nature', 180),\n",
       " ('following', 180),\n",
       " ('villain', 180),\n",
       " ('leaving', 180),\n",
       " ('animated', 179),\n",
       " ('low', 179),\n",
       " ('myself', 179),\n",
       " ('b', 179),\n",
       " ('bill', 179),\n",
       " ('sam', 179),\n",
       " ('filled', 179),\n",
       " ('wars', 179),\n",
       " ('questions', 178),\n",
       " ('cinema', 178),\n",
       " ('message', 178),\n",
       " ('box', 178),\n",
       " ('moving', 178),\n",
       " ('herself', 178),\n",
       " ('country', 178),\n",
       " ('usual', 177),\n",
       " ('martin', 177),\n",
       " ('definitely', 177),\n",
       " ('add', 177),\n",
       " ('large', 177),\n",
       " ('clever', 177),\n",
       " ('create', 177),\n",
       " ('felt', 176),\n",
       " ('stories', 176),\n",
       " ('brilliant', 176),\n",
       " ('ones', 176),\n",
       " ('giant', 176),\n",
       " ('situation', 176),\n",
       " ('murphy', 176),\n",
       " ('break', 175),\n",
       " ('opens', 175),\n",
       " ('scary', 175),\n",
       " ('doubt', 175),\n",
       " ('drug', 175),\n",
       " ('bunch', 174),\n",
       " ('thinking', 174),\n",
       " ('solid', 174),\n",
       " ('effect', 173),\n",
       " ('learn', 172),\n",
       " ('move', 172),\n",
       " ('force', 171),\n",
       " ('potential', 171),\n",
       " ('seriously', 171),\n",
       " ('follows', 170),\n",
       " ('above', 170),\n",
       " ('saying', 170),\n",
       " ('huge', 169),\n",
       " ('class', 169),\n",
       " ('plan', 169),\n",
       " ('agent', 168),\n",
       " ('created', 167),\n",
       " ('unlike', 167),\n",
       " ('pay', 166),\n",
       " ('non', 166),\n",
       " ('married', 166),\n",
       " ('mark', 166),\n",
       " ('sweet', 166),\n",
       " ('perfectly', 166),\n",
       " ('ex', 165),\n",
       " ('realize', 165),\n",
       " ('audiences', 165),\n",
       " ('took', 164),\n",
       " ('decent', 164),\n",
       " ('likely', 164),\n",
       " ('dream', 164),\n",
       " ('view', 164),\n",
       " ('scott', 164),\n",
       " ('subject', 164),\n",
       " ('understand', 164),\n",
       " ('happened', 164),\n",
       " ('enjoyable', 164),\n",
       " ('studio', 163),\n",
       " ('immediately', 163),\n",
       " ('open', 163),\n",
       " ('e', 163),\n",
       " ('points', 163),\n",
       " ('heard', 163),\n",
       " ('viewers', 163),\n",
       " ('cameron', 163),\n",
       " ('truman', 163),\n",
       " ('bruce', 162),\n",
       " ('frank', 162),\n",
       " ('private', 161),\n",
       " ('stay', 161),\n",
       " ('fails', 161),\n",
       " ('impossible', 161),\n",
       " ('cold', 161),\n",
       " ('richard', 161),\n",
       " ('overall', 160),\n",
       " ('merely', 160),\n",
       " ('exciting', 160),\n",
       " ('mess', 159),\n",
       " ('chase', 159),\n",
       " ('free', 159),\n",
       " ('ten', 159),\n",
       " ('neither', 159),\n",
       " ('wanted', 159),\n",
       " ('gun', 159),\n",
       " ('appear', 159),\n",
       " ('carter', 159),\n",
       " ('escape', 158),\n",
       " ('ultimately', 158),\n",
       " ('+', 158),\n",
       " ('fan', 158),\n",
       " ('inside', 157),\n",
       " ('favorite', 157),\n",
       " ('haven', 157),\n",
       " ('modern', 157),\n",
       " ('l', 157),\n",
       " ('wedding', 157),\n",
       " ('stone', 157),\n",
       " ('trek', 157),\n",
       " ('brought', 157),\n",
       " ('trouble', 157),\n",
       " ('otherwise', 156),\n",
       " ('tim', 156),\n",
       " ('5', 156),\n",
       " ('allen', 156),\n",
       " ('bond', 155),\n",
       " ('society', 155),\n",
       " ('liked', 155),\n",
       " ('dumb', 155),\n",
       " ('musical', 154),\n",
       " ('stand', 154),\n",
       " ('political', 154),\n",
       " ('various', 154),\n",
       " ('talented', 154),\n",
       " ('particular', 154),\n",
       " ('west', 154),\n",
       " ('state', 153),\n",
       " ('keeps', 153),\n",
       " ('english', 152),\n",
       " ('silly', 152),\n",
       " ('u', 152),\n",
       " ('situations', 152),\n",
       " ('park', 152),\n",
       " ('teen', 151),\n",
       " ('rating', 151),\n",
       " ('slightly', 151),\n",
       " ('steve', 151),\n",
       " ('truth', 151),\n",
       " ('air', 151),\n",
       " ('element', 150),\n",
       " ('joke', 150),\n",
       " ('spend', 150),\n",
       " ('key', 150),\n",
       " ('biggest', 149),\n",
       " ('members', 149),\n",
       " ('effort', 149),\n",
       " ('government', 149),\n",
       " ('focus', 149),\n",
       " ('eddie', 149),\n",
       " ('soundtrack', 148),\n",
       " ('hands', 148),\n",
       " ('earlier', 148),\n",
       " ('chan', 148),\n",
       " ('purpose', 148),\n",
       " ('today', 148),\n",
       " ('showing', 147),\n",
       " ('memorable', 147),\n",
       " ('six', 147),\n",
       " ('cannot', 147),\n",
       " ('max', 147),\n",
       " ('offers', 147),\n",
       " ('rated', 146),\n",
       " ('mars', 146),\n",
       " ('heavy', 146),\n",
       " ('totally', 146),\n",
       " ('control', 145),\n",
       " ('credit', 145),\n",
       " ('fi', 145),\n",
       " ('woody', 145),\n",
       " ('ideas', 144),\n",
       " ('sci', 144),\n",
       " ('wait', 144),\n",
       " ('sit', 144),\n",
       " ('female', 143),\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_freq_sort = sorted(all_words_freq.items(), key=lambda x:x[1], reverse=True)\n",
    "all_words_freq_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__repr__` should return a printable representation of the object, most likely one of the ways possible to create this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Freq:\n",
    "    def __init__(self, name, freq):\n",
    "        self.name = name\n",
    "        self.freq = freq\n",
    "    def __repr__(self):\n",
    "       # return repr((self.name, self.freq))\n",
    "        return '({}   {})'.format(self.name, self.freq)\n",
    "words_freq = [Word_Freq(item[0], item[1]) for item in all_words_freq.items()]\n",
    "#sorted(Words_Freq, key=lambda word: word.freq, reverse=True)\n",
    "#words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 77717,\n",
       " 'the': 76529,\n",
       " '.': 65876,\n",
       " 'a': 38106,\n",
       " 'and': 35576,\n",
       " 'of': 34123,\n",
       " 'to': 31937,\n",
       " \"'\": 30585,\n",
       " 'is': 25195,\n",
       " 'in': 21822,\n",
       " 's': 18513,\n",
       " '\"': 17612,\n",
       " 'it': 16107,\n",
       " 'that': 15924,\n",
       " '-': 15595}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_15_words = {key[0]:key[1] for key in all_words_freq_sort[:15]}\n",
    "common_15_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words_15 = all_words_freq.most_common(15)\n",
    "#all_words_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(common_15_words.keys(),common_15_words.values(), color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "print(all_words_freq[\"stupid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Converting words to Features with NLTK](https://pythonprogramming.net/words-as-features-nltk-tutorial/?completed=/text-classification-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string #Êù•Êâæpunctuation words\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [w.lower() for w in movie_reviews.words()]\n",
    "all_words_filter = [w for w in all_words if w not in stop_words if w not in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'film': 9517, 'one': 5852, 'movie': 5771, 'like': 3690, 'even': 2565, 'good': 2411, 'time': 2411, 'story': 2169, 'would': 2109, 'much': 2049, ...})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_freq = nltk.FreqDist(all_words_filter)\n",
    "all_words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the most representative 3000 words through all the movie reviews and construct features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_3000 = all_words_freq.most_common(3000)\n",
    "#all_words_3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(item[0] for item in all_words_3000)\n",
    "#word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we're going to build a quick function that will find these top 3,000 words in our positive and negative documents, marking their presence as either positive or negative:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {w:w in words for w in word_features} #dictionary comprehension\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'film': True,\n",
       "  'one': True,\n",
       "  'movie': True,\n",
       "  'like': True,\n",
       "  'even': False,\n",
       "  'good': False,\n",
       "  'time': True,\n",
       "  'story': False,\n",
       "  'would': True,\n",
       "  'much': False,\n",
       "  'character': False,\n",
       "  'also': True,\n",
       "  'get': False,\n",
       "  'two': False,\n",
       "  'well': False,\n",
       "  'characters': False,\n",
       "  'first': False,\n",
       "  '--': False,\n",
       "  'see': True,\n",
       "  'way': True,\n",
       "  'make': False,\n",
       "  'life': True,\n",
       "  'really': True,\n",
       "  'films': False,\n",
       "  'plot': True,\n",
       "  'little': True,\n",
       "  'people': False,\n",
       "  'could': False,\n",
       "  'scene': False,\n",
       "  'man': False,\n",
       "  'bad': True,\n",
       "  'never': False,\n",
       "  'best': False,\n",
       "  'new': True,\n",
       "  'scenes': False,\n",
       "  'many': True,\n",
       "  'director': True,\n",
       "  'know': False,\n",
       "  'movies': True,\n",
       "  'action': False,\n",
       "  'great': False,\n",
       "  'another': True,\n",
       "  'love': True,\n",
       "  'go': False,\n",
       "  'made': True,\n",
       "  'us': True,\n",
       "  'big': True,\n",
       "  'end': False,\n",
       "  'something': False,\n",
       "  'back': False,\n",
       "  'still': False,\n",
       "  'world': False,\n",
       "  'seems': False,\n",
       "  'work': False,\n",
       "  'makes': True,\n",
       "  'however': False,\n",
       "  'every': True,\n",
       "  'though': False,\n",
       "  'better': False,\n",
       "  'real': False,\n",
       "  'audience': False,\n",
       "  'enough': False,\n",
       "  'seen': False,\n",
       "  'take': False,\n",
       "  'around': False,\n",
       "  'going': False,\n",
       "  'year': True,\n",
       "  'performance': False,\n",
       "  'role': False,\n",
       "  'old': False,\n",
       "  'gets': False,\n",
       "  'may': False,\n",
       "  'things': False,\n",
       "  'think': False,\n",
       "  'years': False,\n",
       "  'last': True,\n",
       "  'comedy': True,\n",
       "  'funny': False,\n",
       "  'actually': False,\n",
       "  'long': False,\n",
       "  'look': True,\n",
       "  'almost': False,\n",
       "  'thing': False,\n",
       "  'fact': False,\n",
       "  'nothing': False,\n",
       "  'say': False,\n",
       "  'right': False,\n",
       "  'john': False,\n",
       "  'although': False,\n",
       "  'played': False,\n",
       "  'find': False,\n",
       "  'script': True,\n",
       "  'come': False,\n",
       "  'ever': False,\n",
       "  'cast': False,\n",
       "  'since': False,\n",
       "  'star': True,\n",
       "  'plays': True,\n",
       "  'young': False,\n",
       "  'show': False,\n",
       "  'comes': True,\n",
       "  'part': False,\n",
       "  'original': True,\n",
       "  'actors': False,\n",
       "  'screen': False,\n",
       "  'without': True,\n",
       "  'acting': True,\n",
       "  'three': False,\n",
       "  'day': False,\n",
       "  'point': False,\n",
       "  'lot': False,\n",
       "  'least': True,\n",
       "  'takes': False,\n",
       "  'guy': False,\n",
       "  'quite': False,\n",
       "  'away': False,\n",
       "  'family': False,\n",
       "  'effects': False,\n",
       "  'course': False,\n",
       "  'goes': False,\n",
       "  'minutes': True,\n",
       "  'interesting': False,\n",
       "  'might': False,\n",
       "  'far': False,\n",
       "  'high': False,\n",
       "  'rather': False,\n",
       "  'must': False,\n",
       "  'anything': False,\n",
       "  'place': False,\n",
       "  'set': False,\n",
       "  'yet': False,\n",
       "  'watch': True,\n",
       "  'making': False,\n",
       "  'wife': False,\n",
       "  'hard': False,\n",
       "  'always': False,\n",
       "  'fun': False,\n",
       "  'seem': True,\n",
       "  'special': False,\n",
       "  'bit': False,\n",
       "  'times': False,\n",
       "  'trying': False,\n",
       "  'hollywood': True,\n",
       "  'instead': False,\n",
       "  'give': False,\n",
       "  'want': False,\n",
       "  'picture': True,\n",
       "  'kind': False,\n",
       "  'american': False,\n",
       "  'job': False,\n",
       "  'sense': False,\n",
       "  'woman': False,\n",
       "  'home': False,\n",
       "  'series': False,\n",
       "  'actor': True,\n",
       "  'probably': False,\n",
       "  'help': False,\n",
       "  'half': False,\n",
       "  'along': False,\n",
       "  'men': False,\n",
       "  'everything': False,\n",
       "  'pretty': True,\n",
       "  'becomes': False,\n",
       "  'sure': False,\n",
       "  'black': False,\n",
       "  'together': False,\n",
       "  'dialogue': False,\n",
       "  'money': True,\n",
       "  'become': False,\n",
       "  'gives': False,\n",
       "  'given': True,\n",
       "  'looking': False,\n",
       "  'whole': False,\n",
       "  'watching': False,\n",
       "  'father': False,\n",
       "  'feel': False,\n",
       "  'everyone': False,\n",
       "  'music': False,\n",
       "  'wants': False,\n",
       "  'sex': False,\n",
       "  'less': True,\n",
       "  'done': False,\n",
       "  'horror': False,\n",
       "  'got': False,\n",
       "  'death': False,\n",
       "  'perhaps': False,\n",
       "  'city': False,\n",
       "  'next': False,\n",
       "  'especially': True,\n",
       "  'play': False,\n",
       "  'girl': False,\n",
       "  'mind': False,\n",
       "  '10': False,\n",
       "  'moments': False,\n",
       "  'looks': False,\n",
       "  'completely': True,\n",
       "  '2': False,\n",
       "  'reason': True,\n",
       "  'mother': False,\n",
       "  'whose': False,\n",
       "  'line': False,\n",
       "  'night': False,\n",
       "  'human': False,\n",
       "  'rest': False,\n",
       "  'performances': False,\n",
       "  'different': False,\n",
       "  'evil': False,\n",
       "  'small': False,\n",
       "  'james': False,\n",
       "  'simply': False,\n",
       "  'couple': True,\n",
       "  'put': False,\n",
       "  'let': False,\n",
       "  'anyone': False,\n",
       "  'ending': True,\n",
       "  'case': False,\n",
       "  'several': False,\n",
       "  'dead': False,\n",
       "  'michael': False,\n",
       "  'left': True,\n",
       "  'thought': False,\n",
       "  'school': False,\n",
       "  'shows': False,\n",
       "  'humor': False,\n",
       "  'true': False,\n",
       "  'lost': False,\n",
       "  'written': True,\n",
       "  'friend': False,\n",
       "  'entire': False,\n",
       "  'getting': False,\n",
       "  'town': False,\n",
       "  'turns': False,\n",
       "  'soon': False,\n",
       "  'someone': False,\n",
       "  'second': False,\n",
       "  'main': False,\n",
       "  'stars': True,\n",
       "  'found': False,\n",
       "  'use': False,\n",
       "  'problem': False,\n",
       "  'friends': False,\n",
       "  'tv': False,\n",
       "  'top': False,\n",
       "  'name': True,\n",
       "  'begins': False,\n",
       "  'called': False,\n",
       "  'based': False,\n",
       "  'comic': False,\n",
       "  'david': False,\n",
       "  'head': False,\n",
       "  'else': False,\n",
       "  'idea': False,\n",
       "  'either': False,\n",
       "  'wrong': False,\n",
       "  'unfortunately': False,\n",
       "  'later': False,\n",
       "  'final': True,\n",
       "  'hand': False,\n",
       "  'alien': False,\n",
       "  'house': False,\n",
       "  'group': False,\n",
       "  'full': False,\n",
       "  'used': False,\n",
       "  'tries': False,\n",
       "  'often': False,\n",
       "  'war': False,\n",
       "  'sequence': False,\n",
       "  'keep': True,\n",
       "  'turn': False,\n",
       "  'playing': False,\n",
       "  'boy': False,\n",
       "  'behind': True,\n",
       "  'named': False,\n",
       "  'certainly': False,\n",
       "  'live': False,\n",
       "  'believe': False,\n",
       "  'works': False,\n",
       "  'relationship': False,\n",
       "  'face': False,\n",
       "  'hour': False,\n",
       "  'run': False,\n",
       "  'style': False,\n",
       "  'said': True,\n",
       "  'despite': False,\n",
       "  'person': False,\n",
       "  'finally': False,\n",
       "  'shot': False,\n",
       "  'book': False,\n",
       "  'tell': False,\n",
       "  'maybe': False,\n",
       "  'nice': False,\n",
       "  'son': False,\n",
       "  'perfect': False,\n",
       "  'side': False,\n",
       "  'seeing': False,\n",
       "  'able': False,\n",
       "  'finds': False,\n",
       "  'children': False,\n",
       "  'days': False,\n",
       "  'past': False,\n",
       "  'summer': False,\n",
       "  'camera': False,\n",
       "  'including': False,\n",
       "  'mr': False,\n",
       "  'kids': False,\n",
       "  'lives': False,\n",
       "  'directed': False,\n",
       "  'moment': False,\n",
       "  'game': False,\n",
       "  'running': False,\n",
       "  'fight': False,\n",
       "  'supposed': False,\n",
       "  'video': False,\n",
       "  'car': False,\n",
       "  'matter': False,\n",
       "  'kevin': False,\n",
       "  'joe': True,\n",
       "  'lines': False,\n",
       "  'worth': False,\n",
       "  'daughter': False,\n",
       "  'earth': False,\n",
       "  'starts': False,\n",
       "  'need': False,\n",
       "  'entertaining': False,\n",
       "  'white': False,\n",
       "  'start': False,\n",
       "  'writer': True,\n",
       "  'dark': False,\n",
       "  'short': False,\n",
       "  'self': False,\n",
       "  'worst': True,\n",
       "  'nearly': False,\n",
       "  'opening': False,\n",
       "  'try': False,\n",
       "  'upon': False,\n",
       "  'care': False,\n",
       "  'early': False,\n",
       "  'violence': False,\n",
       "  'throughout': False,\n",
       "  'team': False,\n",
       "  'production': True,\n",
       "  'example': False,\n",
       "  'beautiful': False,\n",
       "  'title': False,\n",
       "  'exactly': False,\n",
       "  'jack': False,\n",
       "  'review': False,\n",
       "  'major': False,\n",
       "  'drama': False,\n",
       "  'problems': False,\n",
       "  'sequences': False,\n",
       "  'obvious': True,\n",
       "  'version': True,\n",
       "  'screenplay': True,\n",
       "  'known': True,\n",
       "  'killer': False,\n",
       "  'robert': False,\n",
       "  'disney': False,\n",
       "  'already': False,\n",
       "  'close': False,\n",
       "  'classic': False,\n",
       "  'others': False,\n",
       "  'hit': False,\n",
       "  'kill': False,\n",
       "  'deep': False,\n",
       "  'five': False,\n",
       "  'order': False,\n",
       "  'act': True,\n",
       "  'simple': False,\n",
       "  'fine': False,\n",
       "  'heart': False,\n",
       "  'roles': False,\n",
       "  'jackie': True,\n",
       "  'direction': False,\n",
       "  'eyes': False,\n",
       "  'four': False,\n",
       "  'question': False,\n",
       "  'sort': False,\n",
       "  'sometimes': False,\n",
       "  'knows': True,\n",
       "  'supporting': True,\n",
       "  'coming': False,\n",
       "  'voice': False,\n",
       "  'women': False,\n",
       "  'truly': False,\n",
       "  'save': True,\n",
       "  'jokes': True,\n",
       "  'computer': False,\n",
       "  'child': False,\n",
       "  'boring': True,\n",
       "  'tom': False,\n",
       "  'level': False,\n",
       "  '1': False,\n",
       "  'body': False,\n",
       "  'guys': False,\n",
       "  'genre': False,\n",
       "  'brother': False,\n",
       "  'strong': False,\n",
       "  'stop': False,\n",
       "  'room': False,\n",
       "  'space': False,\n",
       "  'lee': False,\n",
       "  'ends': False,\n",
       "  'beginning': False,\n",
       "  'ship': False,\n",
       "  'york': False,\n",
       "  'attempt': False,\n",
       "  'thriller': False,\n",
       "  'scream': False,\n",
       "  'peter': False,\n",
       "  'husband': False,\n",
       "  'fiction': False,\n",
       "  'happens': False,\n",
       "  'hero': False,\n",
       "  'novel': False,\n",
       "  'note': False,\n",
       "  'hope': False,\n",
       "  'king': False,\n",
       "  'yes': False,\n",
       "  'says': False,\n",
       "  'tells': False,\n",
       "  'quickly': False,\n",
       "  'romantic': False,\n",
       "  'dog': False,\n",
       "  'oscar': False,\n",
       "  'stupid': True,\n",
       "  'possible': False,\n",
       "  'saw': False,\n",
       "  'lead': True,\n",
       "  'career': False,\n",
       "  'murder': False,\n",
       "  'extremely': False,\n",
       "  'manages': False,\n",
       "  'god': False,\n",
       "  'mostly': True,\n",
       "  'wonder': False,\n",
       "  'particularly': False,\n",
       "  'future': False,\n",
       "  'fans': False,\n",
       "  'sound': False,\n",
       "  'worse': False,\n",
       "  'piece': True,\n",
       "  'involving': False,\n",
       "  'de': False,\n",
       "  'appears': False,\n",
       "  'planet': False,\n",
       "  'paul': False,\n",
       "  'involved': False,\n",
       "  'mean': False,\n",
       "  'none': False,\n",
       "  'taking': False,\n",
       "  'hours': False,\n",
       "  'laugh': False,\n",
       "  'police': False,\n",
       "  'sets': False,\n",
       "  'attention': False,\n",
       "  'co': False,\n",
       "  'hell': False,\n",
       "  'eventually': False,\n",
       "  'single': False,\n",
       "  'fall': False,\n",
       "  'falls': False,\n",
       "  'material': False,\n",
       "  'emotional': False,\n",
       "  'power': False,\n",
       "  'late': False,\n",
       "  'lack': False,\n",
       "  'dr': False,\n",
       "  'van': False,\n",
       "  'result': False,\n",
       "  'elements': False,\n",
       "  'meet': False,\n",
       "  'smith': False,\n",
       "  'science': False,\n",
       "  'experience': False,\n",
       "  'bring': False,\n",
       "  'wild': False,\n",
       "  'living': False,\n",
       "  'theater': False,\n",
       "  'interest': False,\n",
       "  'leads': False,\n",
       "  'word': False,\n",
       "  'feature': False,\n",
       "  'battle': False,\n",
       "  'girls': False,\n",
       "  'alone': False,\n",
       "  'obviously': False,\n",
       "  'george': False,\n",
       "  'within': False,\n",
       "  'usually': False,\n",
       "  'enjoy': False,\n",
       "  'guess': False,\n",
       "  'among': False,\n",
       "  'taken': True,\n",
       "  'feeling': False,\n",
       "  'laughs': False,\n",
       "  'aliens': False,\n",
       "  'talk': False,\n",
       "  'chance': False,\n",
       "  'talent': False,\n",
       "  '3': False,\n",
       "  'middle': False,\n",
       "  'number': False,\n",
       "  'easy': False,\n",
       "  'across': True,\n",
       "  'needs': False,\n",
       "  'attempts': False,\n",
       "  'happen': False,\n",
       "  'television': False,\n",
       "  'chris': False,\n",
       "  'deal': False,\n",
       "  'poor': False,\n",
       "  'form': False,\n",
       "  'girlfriend': False,\n",
       "  'viewer': False,\n",
       "  'release': False,\n",
       "  'killed': False,\n",
       "  'forced': True,\n",
       "  'whether': False,\n",
       "  'wonderful': False,\n",
       "  'feels': False,\n",
       "  'oh': False,\n",
       "  'tale': False,\n",
       "  'serious': False,\n",
       "  'expect': False,\n",
       "  'except': False,\n",
       "  'light': False,\n",
       "  'success': False,\n",
       "  'features': False,\n",
       "  'premise': False,\n",
       "  'happy': False,\n",
       "  'words': False,\n",
       "  'leave': False,\n",
       "  'important': False,\n",
       "  'meets': False,\n",
       "  'history': False,\n",
       "  'giving': False,\n",
       "  'crew': False,\n",
       "  'type': False,\n",
       "  'call': False,\n",
       "  'turned': False,\n",
       "  'released': False,\n",
       "  'parents': False,\n",
       "  'art': False,\n",
       "  'impressive': False,\n",
       "  'mission': False,\n",
       "  'working': False,\n",
       "  'seemed': False,\n",
       "  'score': False,\n",
       "  'told': False,\n",
       "  'recent': False,\n",
       "  'robin': False,\n",
       "  'basically': False,\n",
       "  'entertainment': False,\n",
       "  'america': False,\n",
       "  'surprise': False,\n",
       "  'apparently': False,\n",
       "  'easily': False,\n",
       "  'ryan': False,\n",
       "  'cool': False,\n",
       "  'stuff': False,\n",
       "  'cop': False,\n",
       "  'change': False,\n",
       "  'williams': False,\n",
       "  'crime': False,\n",
       "  'office': False,\n",
       "  'parts': False,\n",
       "  'somehow': False,\n",
       "  'sequel': False,\n",
       "  'william': False,\n",
       "  'cut': True,\n",
       "  'die': False,\n",
       "  'jones': False,\n",
       "  'credits': True,\n",
       "  'batman': False,\n",
       "  'suspense': False,\n",
       "  'brings': False,\n",
       "  'events': False,\n",
       "  'reality': False,\n",
       "  'local': False,\n",
       "  'talking': False,\n",
       "  'difficult': False,\n",
       "  'using': False,\n",
       "  'went': False,\n",
       "  'writing': False,\n",
       "  'remember': True,\n",
       "  'near': False,\n",
       "  'straight': False,\n",
       "  'hilarious': False,\n",
       "  'ago': False,\n",
       "  'certain': False,\n",
       "  'ben': False,\n",
       "  'kid': False,\n",
       "  'slow': False,\n",
       "  'blood': False,\n",
       "  'mystery': False,\n",
       "  'complete': True,\n",
       "  'red': False,\n",
       "  'popular': False,\n",
       "  'effective': False,\n",
       "  'fast': False,\n",
       "  'flick': False,\n",
       "  'due': False,\n",
       "  'runs': False,\n",
       "  'gone': False,\n",
       "  'return': False,\n",
       "  'presence': False,\n",
       "  'quality': False,\n",
       "  'dramatic': False,\n",
       "  'filmmakers': False,\n",
       "  'age': False,\n",
       "  'brothers': False,\n",
       "  'business': False,\n",
       "  'general': False,\n",
       "  'rock': False,\n",
       "  'sexual': False,\n",
       "  'present': False,\n",
       "  'surprisingly': False,\n",
       "  'anyway': False,\n",
       "  'uses': False,\n",
       "  '4': False,\n",
       "  'personal': False,\n",
       "  'figure': False,\n",
       "  'smart': False,\n",
       "  'ways': False,\n",
       "  'decides': False,\n",
       "  'annoying': True,\n",
       "  'begin': False,\n",
       "  'somewhat': False,\n",
       "  'shots': False,\n",
       "  'rich': False,\n",
       "  'minute': False,\n",
       "  'law': False,\n",
       "  'previous': False,\n",
       "  'jim': False,\n",
       "  'successful': False,\n",
       "  'harry': False,\n",
       "  'water': False,\n",
       "  'similar': False,\n",
       "  'absolutely': False,\n",
       "  'motion': False,\n",
       "  'former': False,\n",
       "  'strange': False,\n",
       "  'came': False,\n",
       "  'follow': False,\n",
       "  'read': False,\n",
       "  'project': False,\n",
       "  'million': False,\n",
       "  'secret': False,\n",
       "  'starring': False,\n",
       "  'clear': False,\n",
       "  'familiar': False,\n",
       "  'romance': False,\n",
       "  'intelligent': False,\n",
       "  'third': False,\n",
       "  'excellent': False,\n",
       "  'amazing': False,\n",
       "  'party': False,\n",
       "  'budget': False,\n",
       "  'eye': False,\n",
       "  'actress': False,\n",
       "  'prison': False,\n",
       "  'latest': False,\n",
       "  'means': False,\n",
       "  'company': True,\n",
       "  'towards': False,\n",
       "  'predictable': False,\n",
       "  'powerful': False,\n",
       "  'bob': False,\n",
       "  'beyond': False,\n",
       "  'visual': False,\n",
       "  'leaves': False,\n",
       "  'r': False,\n",
       "  'nature': False,\n",
       "  'following': False,\n",
       "  'villain': False,\n",
       "  'leaving': False,\n",
       "  'animated': False,\n",
       "  'low': False,\n",
       "  'b': False,\n",
       "  'bill': False,\n",
       "  'sam': False,\n",
       "  'filled': False,\n",
       "  'wars': False,\n",
       "  'questions': False,\n",
       "  'cinema': False,\n",
       "  'message': False,\n",
       "  'box': False,\n",
       "  'moving': False,\n",
       "  'country': False,\n",
       "  'usual': False,\n",
       "  'martin': False,\n",
       "  'definitely': False,\n",
       "  'add': False,\n",
       "  'large': False,\n",
       "  'clever': True,\n",
       "  'create': False,\n",
       "  'felt': False,\n",
       "  'stories': False,\n",
       "  'brilliant': False,\n",
       "  'ones': False,\n",
       "  'giant': False,\n",
       "  'situation': False,\n",
       "  'murphy': False,\n",
       "  'break': False,\n",
       "  'opens': False,\n",
       "  'scary': False,\n",
       "  'doubt': False,\n",
       "  'drug': False,\n",
       "  'bunch': False,\n",
       "  'thinking': False,\n",
       "  'solid': False,\n",
       "  'effect': False,\n",
       "  'learn': False,\n",
       "  'move': False,\n",
       "  'force': False,\n",
       "  'potential': False,\n",
       "  'seriously': False,\n",
       "  'follows': False,\n",
       "  'saying': False,\n",
       "  'huge': False,\n",
       "  'class': False,\n",
       "  'plan': False,\n",
       "  'agent': False,\n",
       "  'created': False,\n",
       "  'unlike': False,\n",
       "  'pay': True,\n",
       "  'non': False,\n",
       "  'married': False,\n",
       "  'mark': False,\n",
       "  'sweet': False,\n",
       "  'perfectly': False,\n",
       "  'ex': False,\n",
       "  'realize': False,\n",
       "  'audiences': False,\n",
       "  'took': False,\n",
       "  'decent': False,\n",
       "  'likely': False,\n",
       "  'dream': False,\n",
       "  'view': False,\n",
       "  'scott': False,\n",
       "  'subject': False,\n",
       "  'understand': False,\n",
       "  'happened': False,\n",
       "  'enjoyable': False,\n",
       "  'studio': True,\n",
       "  'immediately': False,\n",
       "  'open': False,\n",
       "  'e': False,\n",
       "  'points': False,\n",
       "  'heard': False,\n",
       "  'viewers': False,\n",
       "  'cameron': False,\n",
       "  'truman': False,\n",
       "  'bruce': True,\n",
       "  'frank': False,\n",
       "  'private': False,\n",
       "  'stay': False,\n",
       "  'fails': False,\n",
       "  'impossible': False,\n",
       "  'cold': False,\n",
       "  'richard': False,\n",
       "  'overall': False,\n",
       "  'merely': False,\n",
       "  'exciting': False,\n",
       "  'mess': True,\n",
       "  'chase': False,\n",
       "  'free': False,\n",
       "  'ten': False,\n",
       "  'neither': False,\n",
       "  'wanted': False,\n",
       "  'gun': False,\n",
       "  'appear': False,\n",
       "  'carter': False,\n",
       "  'escape': False,\n",
       "  'ultimately': False,\n",
       "  'fan': False,\n",
       "  'inside': False,\n",
       "  'favorite': False,\n",
       "  'modern': False,\n",
       "  'l': False,\n",
       "  'wedding': False,\n",
       "  'stone': False,\n",
       "  'trek': False,\n",
       "  'brought': False,\n",
       "  'trouble': False,\n",
       "  'otherwise': True,\n",
       "  'tim': False,\n",
       "  '5': False,\n",
       "  'allen': False,\n",
       "  'bond': False,\n",
       "  'society': False,\n",
       "  'liked': False,\n",
       "  'dumb': False,\n",
       "  'musical': False,\n",
       "  'stand': False,\n",
       "  'political': False,\n",
       "  'various': False,\n",
       "  'talented': False,\n",
       "  'particular': False,\n",
       "  'west': False,\n",
       "  'state': False,\n",
       "  'keeps': False,\n",
       "  'english': False,\n",
       "  'silly': False,\n",
       "  'u': False,\n",
       "  'situations': False,\n",
       "  'park': False,\n",
       "  'teen': False,\n",
       "  'rating': False,\n",
       "  'slightly': False,\n",
       "  'steve': False,\n",
       "  'truth': False,\n",
       "  'air': False,\n",
       "  'element': False,\n",
       "  'joke': False,\n",
       "  'spend': False,\n",
       "  'key': False,\n",
       "  'biggest': False,\n",
       "  'members': False,\n",
       "  'effort': False,\n",
       "  'government': False,\n",
       "  'focus': False,\n",
       "  'eddie': False,\n",
       "  'soundtrack': True,\n",
       "  'hands': False,\n",
       "  'earlier': False,\n",
       "  'chan': True,\n",
       "  'purpose': False,\n",
       "  'today': False,\n",
       "  'showing': False,\n",
       "  'memorable': False,\n",
       "  'six': False,\n",
       "  'cannot': False,\n",
       "  'max': False,\n",
       "  'offers': False,\n",
       "  'rated': False,\n",
       "  'mars': False,\n",
       "  'heavy': False,\n",
       "  'totally': False,\n",
       "  'control': False,\n",
       "  'credit': False,\n",
       "  'fi': False,\n",
       "  'woody': False,\n",
       "  'ideas': False,\n",
       "  'sci': False,\n",
       "  'wait': False,\n",
       "  'sit': False,\n",
       "  'female': False,\n",
       "  'ask': False,\n",
       "  'waste': False,\n",
       "  'terrible': False,\n",
       "  'depth': False,\n",
       "  'simon': False,\n",
       "  'aspect': False,\n",
       "  'list': False,\n",
       "  'mary': False,\n",
       "  'sister': False,\n",
       "  'animation': False,\n",
       "  'entirely': False,\n",
       "  'fear': False,\n",
       "  'steven': False,\n",
       "  'moves': False,\n",
       "  'actual': False,\n",
       "  'army': False,\n",
       "  'british': False,\n",
       "  'constantly': False,\n",
       "  'fire': False,\n",
       "  'convincing': False,\n",
       "  'setting': False,\n",
       "  'gave': False,\n",
       "  'tension': False,\n",
       "  'street': False,\n",
       "  '8': False,\n",
       "  'brief': False,\n",
       "  'ridiculous': False,\n",
       "  'cinematography': False,\n",
       "  'typical': False,\n",
       "  'nick': False,\n",
       "  'screenwriter': False,\n",
       "  'ability': False,\n",
       "  'spent': False,\n",
       "  'quick': False,\n",
       "  'violent': False,\n",
       "  'atmosphere': False,\n",
       "  'subtle': False,\n",
       "  'expected': False,\n",
       "  'fairly': False,\n",
       "  'seven': False,\n",
       "  'killing': False,\n",
       "  'tone': False,\n",
       "  'master': False,\n",
       "  'disaster': False,\n",
       "  'lots': False,\n",
       "  'thinks': False,\n",
       "  'song': True,\n",
       "  'cheap': False,\n",
       "  'suddenly': False,\n",
       "  'background': False,\n",
       "  'club': False,\n",
       "  'willis': True,\n",
       "  'whatever': False,\n",
       "  'highly': False,\n",
       "  'sees': False,\n",
       "  'complex': False,\n",
       "  'greatest': False,\n",
       "  'impact': False,\n",
       "  'beauty': False,\n",
       "  'front': False,\n",
       "  'humans': False,\n",
       "  'indeed': False,\n",
       "  'flat': False,\n",
       "  'grace': False,\n",
       "  'wrote': False,\n",
       "  'amusing': False,\n",
       "  'ii': False,\n",
       "  'mike': True,\n",
       "  'cute': False,\n",
       "  'dull': False,\n",
       "  'minor': False,\n",
       "  'recently': False,\n",
       "  'hate': False,\n",
       "  'outside': False,\n",
       "  'plenty': False,\n",
       "  'wish': False,\n",
       "  'godzilla': False,\n",
       "  'college': False,\n",
       "  'titanic': False,\n",
       "  'sounds': False,\n",
       "  'telling': False,\n",
       "  'sight': False,\n",
       "  'double': False,\n",
       "  'cinematic': False,\n",
       "  'queen': False,\n",
       "  'hold': False,\n",
       "  'meanwhile': False,\n",
       "  'awful': False,\n",
       "  'clearly': False,\n",
       "  'theme': False,\n",
       "  'hear': False,\n",
       "  'x': False,\n",
       "  'amount': False,\n",
       "  'baby': False,\n",
       "  'approach': False,\n",
       "  'dreams': False,\n",
       "  'shown': False,\n",
       "  'island': False,\n",
       "  'reasons': False,\n",
       "  'charm': False,\n",
       "  'miss': False,\n",
       "  'longer': False,\n",
       "  'common': False,\n",
       "  'sean': False,\n",
       "  'carry': False,\n",
       "  'believable': False,\n",
       "  'realistic': False,\n",
       "  'chemistry': False,\n",
       "  'possibly': False,\n",
       "  'casting': False,\n",
       "  'carrey': False,\n",
       "  'french': False,\n",
       "  'trailer': False,\n",
       "  'tough': False,\n",
       "  'produced': False,\n",
       "  'imagine': False,\n",
       "  'choice': False,\n",
       "  'ride': False,\n",
       "  'somewhere': False,\n",
       "  'hot': False,\n",
       "  'race': False,\n",
       "  'road': False,\n",
       "  'leader': False,\n",
       "  'thin': False,\n",
       "  'jerry': True,\n",
       "  'slowly': False,\n",
       "  'delivers': False,\n",
       "  'detective': True,\n",
       "  'brown': False,\n",
       "  'jackson': False,\n",
       "  'member': False,\n",
       "  'provide': False,\n",
       "  'president': False,\n",
       "  'puts': False,\n",
       "  'asks': False,\n",
       "  'critics': False,\n",
       "  'appearance': True,\n",
       "  'famous': False,\n",
       "  'okay': False,\n",
       "  'intelligence': False,\n",
       "  'energy': False,\n",
       "  'sent': True,\n",
       "  'spielberg': False,\n",
       "  'development': False,\n",
       "  'etc': False,\n",
       "  'language': False,\n",
       "  'blue': False,\n",
       "  'proves': False,\n",
       "  'vampire': False,\n",
       "  'seemingly': False,\n",
       "  'basic': False,\n",
       "  ...},\n",
       " 'neg')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Typically the next step is to go ahead and train an algorithm, then test it. So, let's go ahead and do that, starting with the Naive Bayes classifier in the next tutorial!**  \n",
    "[6 Easy Steps to Learn Naive Bayes Algorithm (with codes in Python and R)](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set that we'll train our classifier with\n",
    "training_set = featuresets[:1900]\n",
    "\n",
    "# set that we'll test against.\n",
    "testing_set = featuresets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 88.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     11.1 : 1.0\n",
      "               ludicrous = True              neg : pos    =     10.8 : 1.0\n",
      "                   anger = True              pos : neg    =      9.5 : 1.0\n",
      "                   mulan = True              pos : neg    =      8.9 : 1.0\n",
      "                  finest = True              pos : neg    =      8.0 : 1.0\n",
      "                  seagal = True              neg : pos    =      7.9 : 1.0\n",
      "                   damon = True              pos : neg    =      7.8 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.5 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.1 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.1 : 1.0\n",
      "            breathtaking = True              pos : neg    =      6.2 : 1.0\n",
      "                   flynt = True              pos : neg    =      5.6 : 1.0\n",
      "                lebowski = True              pos : neg    =      5.6 : 1.0\n",
      "                   jolie = True              neg : pos    =      5.4 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Saving Classifiers with NLTK](https://pythonprogramming.net/pickle-classifier-save-nltk-tutorial/?completed=/naive-bayes-classifier-nltk-tutorial/)\n",
    "\n",
    "**Training classifiers and machine learning algorithms can take a very long time, especially if you're training against a larger data set. Ours is actually pretty small. Can you imagine having to train the classifier every time you wanted to fire it up and use it? What horror! Instead, what we can do is use the Pickle module to go ahead and serialize our classifier object, so that all we need to do is load that file in real quick.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This opens up a pickle file, preparing to write in bytes some data. Then, we use pickle.dump() to dump the data. The first parameter to pickle.dump() is what are you dumping, the second parameter is where are you dumping it.\n",
    "\n",
    "After that, we close the file as we're supposed to, and that is that, we now have a pickled, or serialized, object saved in our script's directory!\n",
    "\n",
    "Next, how would we go about opening and using this classifier? The .pickle file is a serialized object, all we need to do now is read it into memory, which will be about as quick as reading any other ordinary file. To do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we do a very similar process. We open the file to read as bytes. Then, we use pickle.load() to load the file, and we save the data to the classifier variable. Then we close the file, and that is that. We now have the same classifier object as before!\n",
    "\n",
    "Now, we can use this object, and we no longer need to train our classifier every time we wanted to use it to classify.\n",
    "\n",
    "While this is all fine and dandy, we're probably not too content with the 60-75% accuracy we're getting. What about other classifiers? Turns out, there are many classifiers, but we need the scikit-learn (sklearn) module. Luckily for us, the people at NLTK recognized the value of incorporating the sklearn module into NLTK, and they have built us a little API to do it. That's what we'll be doing in the next tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Scikit-Learn Sklearn with NLTK](https://pythonprogramming.net/sklearn-scikit-learn-nltk-tutorial/?completed=/pickle-classifier-save-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy percent: 0.88\n",
      "BernoulliNB accuracy percent: 0.88\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MultinomialNB accuracy percent:\",nltk.classify.accuracy(MNB_classifier, testing_set))\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB accuracy percent:\",nltk.classify.accuracy(BNB_classifier, testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 88.0\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     11.1 : 1.0\n",
      "               ludicrous = True              neg : pos    =     10.8 : 1.0\n",
      "                   anger = True              pos : neg    =      9.5 : 1.0\n",
      "                   mulan = True              pos : neg    =      8.9 : 1.0\n",
      "                  finest = True              pos : neg    =      8.0 : 1.0\n",
      "                  seagal = True              neg : pos    =      7.9 : 1.0\n",
      "                   damon = True              pos : neg    =      7.8 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.5 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.1 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.1 : 1.0\n",
      "            breathtaking = True              pos : neg    =      6.2 : 1.0\n",
      "                   flynt = True              pos : neg    =      5.6 : 1.0\n",
      "                lebowski = True              pos : neg    =      5.6 : 1.0\n",
      "                   jolie = True              neg : pos    =      5.4 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.3 : 1.0\n",
      "MNB_classifier accuracy percent: 88.0\n",
      "BernoulliNB_classifier accuracy percent: 88.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 88.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 83.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC_classifier accuracy percent: 81.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 84.0\n",
      "NuSVC_classifier accuracy percent: 89.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "# Áî®SklearnClassifierÊääsklearnÈáåÈù¢ÁöÑclassification modelÂåÖË£πËµ∑Êù•\n",
    "# ÁÑ∂ÂêéÁî®Âà∞nltkÈáåÈù¢Âéª\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see SVC is wrong more often than it is right right out of the gate, so we should probably dump that one. But then what? The next thing we can try is to use all of these algorithms at once. An algo of algos! To do this, we can create another classifier, and make the result of that classifier based on what the other algorithms said. Sort of like a voting system, so we'll just need an odd number of algorithms. That's what we'll be talking about in the next tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Combining Algorithms with NLTK](https://pythonprogramming.net/combine-classifier-algorithms-nltk-tutorial/?completed=/sklearn-scikit-learn-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to use a bunch of algorithmic classifiers, like a child in the candy isle, told they can only pick one, we may find it difficult to choose just one classifier. The good news is, you don't have to! Combining classifier algorithms is a common technique, done by creating a sort of voting system, where each algorithm gets one vote, and the classification that has the votes votes is the chosen one.\n",
    "\n",
    "To do this, we want our new classifier to act like a typical NLTK classifier, with all of the methods. Simple enough, using object oriented programming, we can just be sure to inherit from the NLTK classifier class. To do this, we'll import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes) # returnÈÄâÁ•®ÊúÄÂ§öÁöÑ‰∏ÄÈ°π\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        choice_votes = votes.count(mode(votes))#ÈÄâÁ•®ÊúÄÂ§öÁöÑ‰∏ÄÈ°πÊúâÂ§öÂ∞ë‰∫∫Êäï\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [w.lower() for w in movie_reviews.words()]\n",
    "\n",
    "import string #Êù•Êâæpunctuation words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "stop_words = stopwords.words('english')\n",
    "all_words_filter = [w for w in all_words if w not in punctuations if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_freq = nltk.FreqDist(all_words_filter)\n",
    "all_words_3000 = all_words_freq.most_common(3000)\n",
    "#all_words_3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(w[0] for w in all_words_3000)\n",
    "#word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "#print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featuresets[:1900]\n",
    "testing_set =  featuresets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 96.0\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     11.1 : 1.0\n",
      "               ludicrous = True              neg : pos    =     10.8 : 1.0\n",
      "                   anger = True              pos : neg    =      9.5 : 1.0\n",
      "                   mulan = True              pos : neg    =      8.9 : 1.0\n",
      "                  finest = True              pos : neg    =      8.0 : 1.0\n",
      "                  seagal = True              neg : pos    =      7.9 : 1.0\n",
      "                   damon = True              pos : neg    =      7.8 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.5 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.1 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.1 : 1.0\n",
      "            breathtaking = True              pos : neg    =      6.2 : 1.0\n",
      "                   flynt = True              pos : neg    =      5.6 : 1.0\n",
      "                lebowski = True              pos : neg    =      5.6 : 1.0\n",
      "                   jolie = True              neg : pos    =      5.4 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "classifier_f = open(\"naivebayes.pickle\",\"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", \n",
    "      (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classicfier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent: 90.0\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(MNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB_classifier accuracy percent: 90.0\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 91.0\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 90.0\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC_classifier accuracy percent: 87.0\n"
     ]
    }
   ],
   "source": [
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(SVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 90.0\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy percent: 89.0\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy percent: 92.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: pos Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 85.71428571428571\n",
      "Classification: neg Confidence %: 85.71428571428571\n"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Improving Training Data for sentiment analysis with NLTK](https://pythonprogramming.net/new-data-set-training-nltk-tutorial/?completed=/investigating-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import codecs\n",
    "#f = codecs.open(\"short_reviews/positive.txt\",\n",
    "#                mode=\"r\",\n",
    "#                encoding='utf-8',\n",
    "#                errors='replace')\n",
    "#f.read()\n",
    "\n",
    "short_pos = open(file=\"short_reviews/positive.txt\",\n",
    "                 mode=\"r\",\n",
    "                encoding='utf-8',\n",
    "                errors='replace').read()\n",
    "short_neg = open(file=\"short_reviews/negative.txt\",\n",
    "                 mode=\"r\",\n",
    "                encoding='utf-8',\n",
    "                errors='replace').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of positive reviews: 5332\n",
      "Length of negative reviews: 5332\n"
     ]
    }
   ],
   "source": [
    "positive_documents = [(r, 'pos') for r in short_pos.split('\\n')]\n",
    "negative_documents = [(r, 'neg') for r in short_neg.split('\\n')]\n",
    "print('Length of positive reviews: {}'.format(len(positive_documents)))\n",
    "print('Length of negative reviews: {}'.format(len(negative_documents)))\n",
    "documents = positive_documents + negative_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . ',\n",
       " 'pos')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pos_words = [w.lower() for w in word_tokenize(short_pos)]\n",
    "short_neg_words = [w.lower() for w in word_tokenize(short_neg)]\n",
    "all_words = short_pos_words + short_neg_words\n",
    "\n",
    "import string #Êù•Êâæpunctuation words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "all_words_filter = [w for w in all_words if w not in stop_words if w not in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_freq = nltk.FreqDist(all_words_filter)\n",
    "all_words_5000 = all_words_freq.most_common(5000)\n",
    "#all_words_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\",\n",
       " 'film',\n",
       " 'movie',\n",
       " \"n't\",\n",
       " 'one',\n",
       " 'like',\n",
       " '--',\n",
       " '``',\n",
       " 'story',\n",
       " 'much',\n",
       " 'even',\n",
       " 'good',\n",
       " 'comedy',\n",
       " 'time',\n",
       " 'characters',\n",
       " 'little',\n",
       " 'way',\n",
       " 'funny',\n",
       " 'make',\n",
       " 'enough',\n",
       " 'never',\n",
       " 'director',\n",
       " 'makes',\n",
       " 'would',\n",
       " 'may',\n",
       " 'us',\n",
       " 'work',\n",
       " 'best',\n",
       " 'bad',\n",
       " 'life',\n",
       " 'love',\n",
       " 'could',\n",
       " 'movies',\n",
       " 'new',\n",
       " 'well',\n",
       " 'something',\n",
       " 'really',\n",
       " 'made',\n",
       " 'performances',\n",
       " 'plot',\n",
       " 'many',\n",
       " 'drama',\n",
       " 'films',\n",
       " 'still',\n",
       " 'see',\n",
       " 'look',\n",
       " 'every',\n",
       " 'two',\n",
       " 'people',\n",
       " 'nothing',\n",
       " \"'re\",\n",
       " 'better',\n",
       " 'long',\n",
       " 'without',\n",
       " 'fun',\n",
       " 'get',\n",
       " 'action',\n",
       " 'great',\n",
       " 'though',\n",
       " 'might',\n",
       " 'big',\n",
       " 'also',\n",
       " 'character',\n",
       " 'audience',\n",
       " 'another',\n",
       " 'cast',\n",
       " 'script',\n",
       " 'kind',\n",
       " 'first',\n",
       " 'humor',\n",
       " 'sense',\n",
       " 'world',\n",
       " 'ever',\n",
       " 'performance',\n",
       " 'feels',\n",
       " 'thing',\n",
       " 'far',\n",
       " 'often',\n",
       " 'less',\n",
       " 'seems',\n",
       " 'minutes',\n",
       " 'real',\n",
       " 'picture',\n",
       " 'feel',\n",
       " 'thriller',\n",
       " 'tale',\n",
       " 'almost',\n",
       " 'ca',\n",
       " 'quite',\n",
       " 'documentary',\n",
       " 'yet',\n",
       " 'interesting',\n",
       " \"'ll\",\n",
       " \"'ve\",\n",
       " 'entertaining',\n",
       " 'screen',\n",
       " 'rather',\n",
       " 'man',\n",
       " 'hollywood',\n",
       " 'end',\n",
       " 'take',\n",
       " 'watching',\n",
       " 'seen',\n",
       " 'full',\n",
       " 'go',\n",
       " 'ultimately',\n",
       " 'de',\n",
       " 'hard',\n",
       " 'heart',\n",
       " 'comes',\n",
       " 'romantic',\n",
       " 'moments',\n",
       " 'lot',\n",
       " 'despite',\n",
       " 'american',\n",
       " 'family',\n",
       " 'original',\n",
       " 'year',\n",
       " 'acting',\n",
       " 'old',\n",
       " 'right',\n",
       " 'find',\n",
       " 'worth',\n",
       " 'gets',\n",
       " 'human',\n",
       " 'takes',\n",
       " 'actors',\n",
       " 'times',\n",
       " 'come',\n",
       " 'things',\n",
       " 'dialogue',\n",
       " 'watch',\n",
       " 'back',\n",
       " 'scenes',\n",
       " 'material',\n",
       " 'compelling',\n",
       " 'young',\n",
       " 'music',\n",
       " 'years',\n",
       " 'anyone',\n",
       " 'works',\n",
       " 'kids',\n",
       " 'think',\n",
       " 'want',\n",
       " 'cinema',\n",
       " 'seem',\n",
       " 'least',\n",
       " 'emotional',\n",
       " 'gives',\n",
       " 'going',\n",
       " 'know',\n",
       " 'part',\n",
       " 'say',\n",
       " 'sometimes',\n",
       " 'piece',\n",
       " 'entertainment',\n",
       " 'cinematic',\n",
       " 'subject',\n",
       " 'give',\n",
       " 'last',\n",
       " 'point',\n",
       " 'pretty',\n",
       " 'making',\n",
       " 'special',\n",
       " 'keep',\n",
       " 'bit',\n",
       " 'fascinating',\n",
       " 'away',\n",
       " 'whole',\n",
       " 'together',\n",
       " 'fans',\n",
       " 'dull',\n",
       " 'need',\n",
       " 'anything',\n",
       " 'show',\n",
       " 'since',\n",
       " 'moving',\n",
       " 'style',\n",
       " 'true',\n",
       " 'manages',\n",
       " 'laughs',\n",
       " 'women',\n",
       " 'star',\n",
       " 'offers',\n",
       " 'clever',\n",
       " 'always',\n",
       " 'experience',\n",
       " 'history',\n",
       " 'sweet',\n",
       " 'simply',\n",
       " 'high',\n",
       " 'mr',\n",
       " 'direction',\n",
       " 'everything',\n",
       " 'predictable',\n",
       " 'dark',\n",
       " 'silly',\n",
       " 'instead',\n",
       " 'care',\n",
       " 'title',\n",
       " 'series',\n",
       " 'charm',\n",
       " 'whose',\n",
       " 'actually',\n",
       " 'art',\n",
       " 'flick',\n",
       " 'visual',\n",
       " 'nearly',\n",
       " 'around',\n",
       " 'matter',\n",
       " 'filmmakers',\n",
       " 'place',\n",
       " 'video',\n",
       " 'comic',\n",
       " 'screenplay',\n",
       " 'idea',\n",
       " 'narrative',\n",
       " 'war',\n",
       " 'done',\n",
       " 'short',\n",
       " 'genre',\n",
       " 'goes',\n",
       " 'trying',\n",
       " \"'d\",\n",
       " 'ÔøΩ',\n",
       " 'plays',\n",
       " 'probably',\n",
       " 'familiar',\n",
       " 'children',\n",
       " 'three',\n",
       " 'premise',\n",
       " 'turns',\n",
       " 'horror',\n",
       " 'enjoyable',\n",
       " 'lacks',\n",
       " 'engaging',\n",
       " 'effects',\n",
       " 'home',\n",
       " 'becomes',\n",
       " 'set',\n",
       " 'although',\n",
       " 'smart',\n",
       " 'especially',\n",
       " 'worst',\n",
       " 'feeling',\n",
       " 'wo',\n",
       " 'feature',\n",
       " 'enjoy',\n",
       " 'power',\n",
       " 'amusing',\n",
       " 'effort',\n",
       " 'ending',\n",
       " 'strong',\n",
       " 'charming',\n",
       " 'intelligent',\n",
       " 'half',\n",
       " 'day',\n",
       " 'study',\n",
       " 'likely',\n",
       " 'romance',\n",
       " 'debut',\n",
       " 'theater',\n",
       " 'lack',\n",
       " 'boring',\n",
       " 'mostly',\n",
       " 'put',\n",
       " 'john',\n",
       " 'men',\n",
       " 'version',\n",
       " 'portrait',\n",
       " 'mind',\n",
       " 'filmmaker',\n",
       " 'else',\n",
       " 'certainly',\n",
       " 'beautiful',\n",
       " 'sort',\n",
       " 'everyone',\n",
       " 'easy',\n",
       " 'beautifully',\n",
       " 'next',\n",
       " 'level',\n",
       " 'become',\n",
       " 'solid',\n",
       " 'wit',\n",
       " 'message',\n",
       " 'looking',\n",
       " 'surprisingly',\n",
       " 'rare',\n",
       " 'lives',\n",
       " 'play',\n",
       " 'problem',\n",
       " 'quirky',\n",
       " 'fine',\n",
       " 'actor',\n",
       " 'along',\n",
       " 'hours',\n",
       " 'exercise',\n",
       " 'fresh',\n",
       " 'ideas',\n",
       " 'whether',\n",
       " 'obvious',\n",
       " 'viewers',\n",
       " 'face',\n",
       " 'sure',\n",
       " 'directed',\n",
       " 'leave',\n",
       " 'looks',\n",
       " 'mess',\n",
       " 'either',\n",
       " 'energy',\n",
       " 'powerful',\n",
       " 'completely',\n",
       " 'modern',\n",
       " 'past',\n",
       " 'french',\n",
       " 'interest',\n",
       " 'culture',\n",
       " 'reason',\n",
       " 'shot',\n",
       " 'dramatic',\n",
       " 'neither',\n",
       " \"'m\",\n",
       " 'melodrama',\n",
       " 'classic',\n",
       " 'filmmaking',\n",
       " 'beyond',\n",
       " 'fact',\n",
       " 'recent',\n",
       " 'suspense',\n",
       " 'stuff',\n",
       " 'shows',\n",
       " 'tone',\n",
       " 'delivers',\n",
       " 'fails',\n",
       " 'book',\n",
       " 'slow',\n",
       " 'truly',\n",
       " 'deeply',\n",
       " 'intriguing',\n",
       " 'left',\n",
       " 'believe',\n",
       " 'tv',\n",
       " 'ride',\n",
       " 'death',\n",
       " 'tries',\n",
       " 'serious',\n",
       " 'ends',\n",
       " 'jokes',\n",
       " 'spirit',\n",
       " 'production',\n",
       " 'summer',\n",
       " 'black',\n",
       " 'opera',\n",
       " 'occasionally',\n",
       " 'light',\n",
       " 'girl',\n",
       " 'reality',\n",
       " 'adventure',\n",
       " 'sad',\n",
       " 'touching',\n",
       " 'dumb',\n",
       " 'audiences',\n",
       " 'project',\n",
       " 'small',\n",
       " 'role',\n",
       " 'exactly',\n",
       " 'formula',\n",
       " 'remains',\n",
       " 'terrific',\n",
       " 'hilarious',\n",
       " 'satisfying',\n",
       " 'got',\n",
       " 'proves',\n",
       " 'passion',\n",
       " 'woman',\n",
       " 'storytelling',\n",
       " 'camera',\n",
       " 'different',\n",
       " 'seeing',\n",
       " 'stories',\n",
       " 'scene',\n",
       " 'must',\n",
       " 'impossible',\n",
       " 'line',\n",
       " 'images',\n",
       " 'talent',\n",
       " 'already',\n",
       " 'journey',\n",
       " 'personal',\n",
       " 'particularly',\n",
       " 'attempt',\n",
       " 'easily',\n",
       " 'simple',\n",
       " 'sex',\n",
       " 'gags',\n",
       " 'honest',\n",
       " 'disney',\n",
       " 'turn',\n",
       " 'complex',\n",
       " 'psychological',\n",
       " 'perfect',\n",
       " 'close',\n",
       " 'animation',\n",
       " 'pleasure',\n",
       " 'given',\n",
       " 'hour',\n",
       " 'pretentious',\n",
       " 'case',\n",
       " 'michael',\n",
       " 'intelligence',\n",
       " 'difficult',\n",
       " 'ways',\n",
       " 'visually',\n",
       " 'earnest',\n",
       " 'found',\n",
       " 'falls',\n",
       " 'flat',\n",
       " 'mystery',\n",
       " 'coming-of-age',\n",
       " 'sequel',\n",
       " 'head',\n",
       " 'social',\n",
       " 'cliches',\n",
       " 'boy',\n",
       " 'getting',\n",
       " 'political',\n",
       " 'leaves',\n",
       " 'thought',\n",
       " 'game',\n",
       " 'novel',\n",
       " 'que',\n",
       " 'violence',\n",
       " 'uses',\n",
       " 'crime',\n",
       " 'tell',\n",
       " 'live',\n",
       " 'none',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'memorable',\n",
       " 'help',\n",
       " 'gone',\n",
       " 'finally',\n",
       " 'told',\n",
       " 'keeps',\n",
       " 'side',\n",
       " 'brilliant',\n",
       " 'job',\n",
       " 'laugh',\n",
       " 'rich',\n",
       " 'wrong',\n",
       " 'satire',\n",
       " 'overall',\n",
       " 'days',\n",
       " 'barely',\n",
       " 'otherwise',\n",
       " 'hero',\n",
       " 'elements',\n",
       " 'viewer',\n",
       " 'acted',\n",
       " 'needs',\n",
       " 'spy',\n",
       " 'second',\n",
       " 'rarely',\n",
       " 'guys',\n",
       " 'lost',\n",
       " 'contrived',\n",
       " 'cool',\n",
       " 'cold',\n",
       " 'thoughtful',\n",
       " 'possible',\n",
       " 'nature',\n",
       " 'lead',\n",
       " 'taste',\n",
       " 'several',\n",
       " 'wild',\n",
       " 'appeal',\n",
       " 'starts',\n",
       " 'entirely',\n",
       " 'final',\n",
       " 'approach',\n",
       " 'imagine',\n",
       " 'rock',\n",
       " 'insight',\n",
       " 'perhaps',\n",
       " 'act',\n",
       " 'eyes',\n",
       " 'excellent',\n",
       " 'surprising',\n",
       " 'running',\n",
       " 'david',\n",
       " 'wonderful',\n",
       " 'creepy',\n",
       " 'truth',\n",
       " 'others',\n",
       " 'form',\n",
       " 'tragedy',\n",
       " 'warm',\n",
       " 'mood',\n",
       " 'moral',\n",
       " 'fairly',\n",
       " 'concept',\n",
       " 'soap',\n",
       " 'quality',\n",
       " 'attention',\n",
       " 'behind',\n",
       " 'thoroughly',\n",
       " 'entire',\n",
       " 'engrossing',\n",
       " 'teen',\n",
       " 'parents',\n",
       " 'among',\n",
       " 'comedies',\n",
       " 'result',\n",
       " 'imagination',\n",
       " 'call',\n",
       " 'remarkable',\n",
       " 'expect',\n",
       " 'bring',\n",
       " 'adults',\n",
       " 'adaptation',\n",
       " 'sci-fi',\n",
       " 'moment',\n",
       " 'tedious',\n",
       " 'vision',\n",
       " 'start',\n",
       " 'genuine',\n",
       " 'four',\n",
       " 'nice',\n",
       " 'epic',\n",
       " 'perfectly',\n",
       " 'la',\n",
       " 'animated',\n",
       " 'writer-director',\n",
       " 'quiet',\n",
       " 'merely',\n",
       " 'sharp',\n",
       " 'period',\n",
       " 'maybe',\n",
       " 'strange',\n",
       " 'knows',\n",
       " 'add',\n",
       " 'change',\n",
       " 'usual',\n",
       " 'emotionally',\n",
       " 'hope',\n",
       " 'gorgeous',\n",
       " 'witty',\n",
       " 'future',\n",
       " 'bland',\n",
       " 'remake',\n",
       " 'latest',\n",
       " 'tired',\n",
       " 'effective',\n",
       " 'beauty',\n",
       " 'points',\n",
       " 'someone',\n",
       " 'begins',\n",
       " 'gentle',\n",
       " 'plenty',\n",
       " 'somewhat',\n",
       " 'offer',\n",
       " 'impressive',\n",
       " 'events',\n",
       " 'worthy',\n",
       " 'taking',\n",
       " 'depth',\n",
       " 'thanks',\n",
       " 'wonder',\n",
       " 'appealing',\n",
       " 'deep',\n",
       " 'ii',\n",
       " 'dead',\n",
       " 'worse',\n",
       " 'provides',\n",
       " 'air',\n",
       " 'however',\n",
       " 'unfortunately',\n",
       " 'captures',\n",
       " 'career',\n",
       " 'surprise',\n",
       " 'try',\n",
       " '2',\n",
       " 'pure',\n",
       " 'important',\n",
       " 'pictures',\n",
       " 'age',\n",
       " 'emotions',\n",
       " 'hit',\n",
       " 'definitely',\n",
       " 'sit',\n",
       " 'fantasy',\n",
       " 'guy',\n",
       " 'clear',\n",
       " 'lots',\n",
       " 'stupid',\n",
       " 'straight',\n",
       " 'decent',\n",
       " 'ambitious',\n",
       " 'awful',\n",
       " 'utterly',\n",
       " 'brings',\n",
       " 'pace',\n",
       " 'grant',\n",
       " 'sound',\n",
       " 'run',\n",
       " 'america',\n",
       " 'view',\n",
       " 'exciting',\n",
       " 'execution',\n",
       " 'school',\n",
       " 'sequences',\n",
       " 'magic',\n",
       " 'welcome',\n",
       " 'throughout',\n",
       " 'able',\n",
       " 'subtle',\n",
       " 'historical',\n",
       " 'inside',\n",
       " 'scary',\n",
       " 'highly',\n",
       " 'city',\n",
       " 'fire',\n",
       " 'suffers',\n",
       " 'robert',\n",
       " 'ugly',\n",
       " 'thin',\n",
       " 'provocative',\n",
       " 'lovely',\n",
       " 'cheap',\n",
       " 'ensemble',\n",
       " 'sexual',\n",
       " 'process',\n",
       " 'sustain',\n",
       " 'memory',\n",
       " 'female',\n",
       " 'coming',\n",
       " 'allen',\n",
       " 'deserves',\n",
       " 'certain',\n",
       " 'examination',\n",
       " 'read',\n",
       " 'based',\n",
       " 'alone',\n",
       " 'artist',\n",
       " 'relationship',\n",
       " 'creative',\n",
       " 'cartoon',\n",
       " 'words',\n",
       " 'question',\n",
       " 'working',\n",
       " 'master',\n",
       " 'cute',\n",
       " 'bond',\n",
       " 'potential',\n",
       " 'felt',\n",
       " 'quickly',\n",
       " 'impact',\n",
       " 'deal',\n",
       " 'hand',\n",
       " 'delightful',\n",
       " 'flaws',\n",
       " 'playing',\n",
       " 'middle',\n",
       " 'williams',\n",
       " 'wants',\n",
       " 'poignant',\n",
       " 'major',\n",
       " 'upon',\n",
       " 'use',\n",
       " 'creates',\n",
       " 'used',\n",
       " 'across',\n",
       " 'chemistry',\n",
       " 'except',\n",
       " 'let',\n",
       " 'tension',\n",
       " 'low',\n",
       " 'single',\n",
       " 'hell',\n",
       " 'rest',\n",
       " 'actress',\n",
       " 'sensitive',\n",
       " 'filled',\n",
       " 'lacking',\n",
       " 'talented',\n",
       " 'force',\n",
       " 'winning',\n",
       " 'unexpected',\n",
       " 'taken',\n",
       " 'sentimental',\n",
       " 'spielberg',\n",
       " 'cultural',\n",
       " 'flawed',\n",
       " 'stand',\n",
       " 'yes',\n",
       " 'era',\n",
       " 'unsettling',\n",
       " 'murder',\n",
       " 'odd',\n",
       " 'success',\n",
       " 'situation',\n",
       " 'puts',\n",
       " 'touch',\n",
       " 'waste',\n",
       " 'mediocre',\n",
       " 'generic',\n",
       " 'loud',\n",
       " 'unfunny',\n",
       " 'jackson',\n",
       " 'surprises',\n",
       " 'eye',\n",
       " 'melodramatic',\n",
       " 'extremely',\n",
       " 'crush',\n",
       " 'heavy',\n",
       " 'mark',\n",
       " 'e',\n",
       " 'dog',\n",
       " 'hold',\n",
       " 'ability',\n",
       " 'sincere',\n",
       " 'create',\n",
       " 'relationships',\n",
       " 'previous',\n",
       " 'ultimate',\n",
       " 'george',\n",
       " 'giving',\n",
       " 'slightly',\n",
       " 'apart',\n",
       " 'issues',\n",
       " 'remember',\n",
       " 'couple',\n",
       " 'reveals',\n",
       " 'niro',\n",
       " 'watchable',\n",
       " 'casting',\n",
       " 'class',\n",
       " 'weird',\n",
       " 'formulaic',\n",
       " 'leads',\n",
       " 'supposed',\n",
       " 'house',\n",
       " 'cut',\n",
       " 'mildly',\n",
       " 'convincing',\n",
       " 'hardly',\n",
       " 'ago',\n",
       " 'country',\n",
       " 'college',\n",
       " 'saw',\n",
       " 'seriously',\n",
       " 'routine',\n",
       " 'steven',\n",
       " 'road',\n",
       " 'forgettable',\n",
       " 'finds',\n",
       " 'pleasant',\n",
       " 'involved',\n",
       " 'living',\n",
       " 'five',\n",
       " 'indeed',\n",
       " 'extreme',\n",
       " 'stars',\n",
       " 'date',\n",
       " 'attempts',\n",
       " 'girls',\n",
       " 'treat',\n",
       " 'slight',\n",
       " 'joke',\n",
       " 'gripping',\n",
       " 'fully',\n",
       " 'ones',\n",
       " 'inventive',\n",
       " 'intimate',\n",
       " 'trouble',\n",
       " 'largely',\n",
       " 'grace',\n",
       " 'writer',\n",
       " 'twists',\n",
       " 'thinking',\n",
       " 'course',\n",
       " 'terms',\n",
       " 'amount',\n",
       " 'focus',\n",
       " 'crazy',\n",
       " 'uneven',\n",
       " 'old-fashioned',\n",
       " 'tragic',\n",
       " 'thrills',\n",
       " 'mix',\n",
       " 'contemporary',\n",
       " 'pop',\n",
       " 'episode',\n",
       " 'water',\n",
       " 'stylish',\n",
       " '2002',\n",
       " 'money',\n",
       " 'sets',\n",
       " 'problems',\n",
       " 'succeeds',\n",
       " 'york',\n",
       " 'plain',\n",
       " 'pacing',\n",
       " 'masterpiece',\n",
       " 'disturbing',\n",
       " 'unique',\n",
       " 'happy',\n",
       " 'called',\n",
       " 'es',\n",
       " 'substance',\n",
       " 'happens',\n",
       " 'painful',\n",
       " 'boys',\n",
       " 'flicks',\n",
       " 'goofy',\n",
       " 'friendship',\n",
       " 'business',\n",
       " 'presents',\n",
       " 'absolutely',\n",
       " 'complete',\n",
       " 'meditation',\n",
       " 'involving',\n",
       " 'poetry',\n",
       " 'monster',\n",
       " 'recommend',\n",
       " 'successful',\n",
       " 'runs',\n",
       " 'colorful',\n",
       " 'themes',\n",
       " 'promise',\n",
       " 'appears',\n",
       " 'considerable',\n",
       " 'company',\n",
       " 'soundtrack',\n",
       " 'heaven',\n",
       " 'drag',\n",
       " 'white',\n",
       " 'missing',\n",
       " 'badly',\n",
       " 'somehow',\n",
       " 'kid',\n",
       " 'loses',\n",
       " 'target',\n",
       " 'hits',\n",
       " 'british',\n",
       " 'triumph',\n",
       " 'absorbing',\n",
       " 'imax',\n",
       " 'clichÔøΩs',\n",
       " 'refreshing',\n",
       " 'manner',\n",
       " 'parts',\n",
       " 'originality',\n",
       " 'urban',\n",
       " 'played',\n",
       " 'soul',\n",
       " 'general',\n",
       " 'person',\n",
       " 'equally',\n",
       " 'forget',\n",
       " 'typical',\n",
       " 'politics',\n",
       " 'interested',\n",
       " 'strangely',\n",
       " 'murphy',\n",
       " 'mean',\n",
       " 'terrible',\n",
       " 'pieces',\n",
       " 'sandler',\n",
       " 'blend',\n",
       " 'bizarre',\n",
       " 'wanted',\n",
       " 'oscar',\n",
       " 'skin',\n",
       " 'share',\n",
       " 'franchise',\n",
       " 'bright',\n",
       " 'sentimentality',\n",
       " 'effect',\n",
       " 'gay',\n",
       " 'tom',\n",
       " 'evil',\n",
       " 'green',\n",
       " 'viewing',\n",
       " 'situations',\n",
       " 'word',\n",
       " 'forced',\n",
       " 'hate',\n",
       " 'standard',\n",
       " 'fare',\n",
       " 'means',\n",
       " 'el',\n",
       " 'today',\n",
       " 'whatever',\n",
       " 'alive',\n",
       " 'depressing',\n",
       " 'grief',\n",
       " 'sophisticated',\n",
       " 'battle',\n",
       " 'somewhere',\n",
       " 'obviously',\n",
       " 'well-acted',\n",
       " 'understand',\n",
       " 'conflict',\n",
       " 'painfully',\n",
       " 'frame',\n",
       " 'please',\n",
       " 'wedding',\n",
       " 'energetic',\n",
       " 'soderbergh',\n",
       " 'sexy',\n",
       " 'miss',\n",
       " 'report',\n",
       " 'liked',\n",
       " 'stunning',\n",
       " 'night',\n",
       " 'dry',\n",
       " 'average',\n",
       " 'setting',\n",
       " 'fast',\n",
       " 'fan',\n",
       " 'questions',\n",
       " 'haunting',\n",
       " 'television',\n",
       " 'created',\n",
       " 'dream',\n",
       " 'central',\n",
       " 'loss',\n",
       " 'tells',\n",
       " 'name',\n",
       " 'eventually',\n",
       " 'virtually',\n",
       " 'twist',\n",
       " 'derivative',\n",
       " 'crafted',\n",
       " 'doubt',\n",
       " 'said',\n",
       " 'lame',\n",
       " '90',\n",
       " 'poor',\n",
       " 'huge',\n",
       " 'peter',\n",
       " 'refreshingly',\n",
       " 'amazing',\n",
       " 'treasure',\n",
       " 'ms',\n",
       " 'riveting',\n",
       " 'imaginative',\n",
       " 'footage',\n",
       " 'blue',\n",
       " 'excitement',\n",
       " 'allows',\n",
       " 'unlike',\n",
       " 'humanity',\n",
       " 'lines',\n",
       " 'heavy-handed',\n",
       " 'insightful',\n",
       " 'possibly',\n",
       " 'moore',\n",
       " 'parker',\n",
       " 'remarkably',\n",
       " 'affecting',\n",
       " 'warmth',\n",
       " 'generation',\n",
       " 'thought-provoking',\n",
       " 'holds',\n",
       " 'fat',\n",
       " 'brain',\n",
       " 'fiction',\n",
       " 'price',\n",
       " 'weak',\n",
       " 'tough',\n",
       " 'places',\n",
       " 'vivid',\n",
       " 'break',\n",
       " 'toward',\n",
       " 'slapstick',\n",
       " 'x',\n",
       " 'deliver',\n",
       " 'inspired',\n",
       " 'genuinely',\n",
       " 'faith',\n",
       " 'atmosphere',\n",
       " 'generally',\n",
       " 'opportunity',\n",
       " 'conventional',\n",
       " 'finish',\n",
       " 'clearly',\n",
       " 'una',\n",
       " 'sea',\n",
       " 'screenwriter',\n",
       " 'match',\n",
       " 'worthwhile',\n",
       " 'japanese',\n",
       " 'popcorn',\n",
       " 'edge',\n",
       " 'red',\n",
       " 'room',\n",
       " 'lets',\n",
       " ...]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features = [w[0] for w in all_words_5000]\n",
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(featuresets)\n",
    "#state = [feature[1] for feature in featuresets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featuresets[:10000]\n",
    "testing_set =  featuresets[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayes_classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 0.7831325301204819\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_accuracy = nltk.classify.accuracy(NaiveBayes_classifier, testing_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent: {}\".format(naive_bayes_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    flat = True              neg : pos    =     24.4 : 1.0\n",
      "              engrossing = True              pos : neg    =     19.6 : 1.0\n",
      "               wonderful = True              pos : neg    =     18.3 : 1.0\n",
      "                 generic = True              neg : pos    =     16.4 : 1.0\n",
      "                 routine = True              neg : pos    =     16.4 : 1.0\n",
      "                mediocre = True              neg : pos    =     15.7 : 1.0\n",
      "               inventive = True              pos : neg    =     15.6 : 1.0\n",
      "                  boring = True              neg : pos    =     14.5 : 1.0\n",
      "                intimate = True              pos : neg    =     13.6 : 1.0\n",
      "              refreshing = True              pos : neg    =     13.6 : 1.0\n",
      "                      90 = True              neg : pos    =     13.0 : 1.0\n",
      "                    warm = True              pos : neg    =     13.0 : 1.0\n",
      "             mesmerizing = True              pos : neg    =     11.6 : 1.0\n",
      "                mindless = True              neg : pos    =     11.0 : 1.0\n",
      "                  stupid = True              neg : pos    =     11.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "NaiveBayes_classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent: 78.46385542168674\n",
      "BernoulliNB_classifier accuracy percent: 78.76506024096386\n",
      "LogisticRegression_classifier accuracy percent: 75.6024096385542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 73.04216867469879\n",
      "LinearSVC_classifier accuracy percent: 72.28915662650603\n",
      "NuSVC_classifier accuracy percent: 75.30120481927712\n",
      "voted_classifier accuracy percent: 76.95783132530121\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "##SVC_classifier = SklearnClassifier(SVC())\n",
    "##SVC_classifier.train(training_set)\n",
    "##print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "\n",
    "voted_classifier = VoteClassifier(\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [A dive into Natural Language Processing](https://medium.com/greyatom/a-dive-into-natural-language-processing-103ae9b0a588)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('corpus_long.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus_long.txt'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(fo)\n",
    "text.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Line: PREFACE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Read Line: %s'%text.readline(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = text.read()\n",
    "sents = nltk.sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SUPPOSING that Truth is a woman--what then?\n",
      "Is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists, have failed to understand women--that the terrible\n",
      "seriousness and clumsy importunity with which they have usually paid\n",
      "their addresses to Truth, have been unskilled and unseemly methods for\n",
      "winning a woman?\n",
      "Certainly she has never allowed herself to be won; and\n",
      "at present every kind of dogma stands with sad and discouraged mien--IF,\n",
      "indeed, it stands at all!\n"
     ]
    }
   ],
   "source": [
    "#len(sents)\n",
    "print(sents[0])\n",
    "print(sents[1])\n",
    "print(sents[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SUPPOSING that Truth is a woman--what then?\n",
      "Is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists, have failed to understand women--that the terrible\n",
      "seriousness and clumsy importunity with which they have usually paid\n",
      "their addresses to Truth, have been unskilled and unseemly methods for\n",
      "winning a woman?\n",
      "Certainly she has never allowed herself to be won; and\n",
      "at present every kind of dogma stands with sad and discouraged mien--IF,\n",
      "indeed, it stands at all!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(sents[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUPPOSING', 'that', 'Truth', 'is', 'a', 'woman', 'what', 'then']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(sents[0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'itself', 'can', 'as', 'i', 'you', 'her', 'having', 'if', \"hasn't\", 'needn', 'had', 'other', \"isn't\", 'myself', \"don't\", 'to', 'now', 'shan', 'is', \"mustn't\", 'hers', 'which', 'above', 'against', 'for', 'wasn', 'who', 'until', 'will', \"didn't\", \"couldn't\", 'ma', 'herself', 'they', 'theirs', 'be', 'isn', \"you'd\", 'down', 'its', 'that', 'again', 'we', 'some', 'did', 'from', 'were', 'on', \"she's\", 'each', 'won', \"wouldn't\", 'our', \"hadn't\", 'yours', 'before', 'am', 'does', 'during', 'or', \"you'll\", 'while', 'mustn', 'their', 'ain', 'just', 'under', 'over', 'couldn', 'such', 'a', 'below', 'but', 'further', 'he', 'off', \"you're\", 'at', 'those', 'all', 'was', 'through', 'my', 'no', \"weren't\", 'with', 'aren', 'how', 'not', 's', 'it', 'own', 'haven', 'don', 'd', 'out', 'most', 'only', 'yourself', 'ours', 'very', 'hadn', 'too', 'has', 'because', 'm', 'have', 'yourselves', 'them', 'whom', 'are', 'll', 've', \"aren't\", 'what', \"haven't\", 're', 'and', 'hasn', 'mightn', 'after', \"wasn't\", 'didn', 'here', 'do', 'why', 'been', 'this', 'o', 'so', \"needn't\", 'of', \"doesn't\", 'once', \"it's\", 'she', 'himself', 'between', 'than', 'more', 'there', \"that'll\", 'themselves', 'y', 'where', 'about', 'wouldn', 'him', 'being', 'then', 'both', 'his', 'these', 'in', 'doesn', \"you've\", 'same', 'shouldn', 'when', \"mightn't\", 'your', \"should've\", 'few', 'the', 'into', 'weren', 'up', 't', 'by', \"shan't\", \"won't\", 'me', 'any', 'ourselves', 'an', 'nor', \"shouldn't\", 'doing', 'should'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUPPOSING', 'that', 'Truth', 'is', 'a', 'woman', 'what', 'then']\n",
      "=================================================================================\n",
      "['SUPPOSING', 'Truth', 'woman']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = []\n",
    "for word in tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_sentence.append(word)\n",
    "print(tokens)\n",
    "print(\"=================================================================================\")\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUPPOSING', 'Truth', 'woman']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Áî®list comprehensionÂ≤Ç‰∏çÊòØÊõ¥Â•Ω\n",
    "filtered_tokens = list(filter(lambda x: x not in stop_words, tokens))\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming And Lemmatization\n",
    "Some words represent the same meaning. For example, Copy, copied, copying. The model might treat them differently, so we tend to strip such words to their core. We can do that by stemming or lemmatisation. Stemming and Lemmatization are the basic text processing methods for English text.\n",
    "#### Stemming\n",
    "It helps to create groups of words which have similar meanings and works based on a set of rules, such as remove ‚Äúing‚Äù if words are ending with ‚Äúing‚Äù. Different types of stemmers in NLTK are PorterStemmer, LancasterStemmer, SnowballStemmer.\n",
    "#### Lemmatization\n",
    "It uses a knowledgebase called WordNet. Because of knowledge, lemmatization can even convert words which are different and cant be solved by stemmers, for example converting ‚Äúcame‚Äù to ‚Äúcome‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cri'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----PorterStemmer------\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem(\"crying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----LancasterStemmer------\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lancaster_stemmer.stem(\"crying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cri'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----SnowballStemmer------#-----Sn \n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "snowball_stemmer.stem(\"crying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(\"cried\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an NLP Pipeline, Step-by-Step\n",
    "* [ValueError: unknown locale: UTF-8](https://stackoverflow.com/questions/15526996/ipython-notebook-locale-error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.16\n"
     ]
    }
   ],
   "source": [
    "# Install spaCy \n",
    "#!pip3 install -U spacy\n",
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the large English model for spaCy\n",
    "#!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install textacy which will also be useful\n",
    "#!pip3 install -U textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom.  Standing on the River Thames in the south east \n",
    "of the island of Great Britain, London has been a major settlement \n",
    "for two millennia. It was founded by the Romans, who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the text with spaCy. This runs the entire pipeline.\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "London is the capital and most populous city of England and \n",
       "the United Kingdom.  Standing on the River Thames in the south east \n",
       "of the island of Great Britain, London has been a major settlement \n",
       "for two millennia. It was founded by the Romans, who named it Londinium."
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London     (GPE)\n",
      "England     (GPE)\n",
      "the United Kingdom     (GPE)\n",
      "the River Thames     (FAC)\n",
      "Great Britain     (GPE)\n",
      "London     (GPE)\n",
      "two millennia     (DATE)\n",
      "Romans     (NORP)\n",
      "Londinium     (PERSON)\n"
     ]
    }
   ],
   "source": [
    "# 'doc' now contains a parsed version of text. We can use it to do anything we want!\n",
    "# For example, this will print out all the named entities that were detected:\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text}     ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "England\n",
      "the United Kingdom\n",
      "the River Thames\n",
      "Great Britain\n",
      "London\n",
      "two millennia\n",
      "Romans\n",
      "Londinium\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    #print(ent.merge())\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function merge:\n",
      "\n",
      "merge(...) method of spacy.tokens.span.Span instance\n",
      "    Retokenize the document, such that the span is merged into a single\n",
      "    token.\n",
      "    \n",
      "    **attributes: Attributes to assign to the merged token. By default,\n",
      "        attributes are inherited from the syntactic root token of the span.\n",
      "    RETURNS (Token): The newly merged token.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ent.merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Replace a token with \"REDACTED\" if it is a name\n",
    "def replace_name_with_placeholder(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED] \"\n",
    "    else:\n",
    "        return token.string\n",
    "\n",
    "# Loop through all the entities in a document and check if they are names\n",
    "def scrub(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        ent.merge()\n",
    "    tokens = map(replace_name_with_placeholder, doc)\n",
    "    return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\". In 1957, Noam Chomsky‚Äôs \n",
    "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In 1950, [REDACTED] published his famous article \"Computing Machinery and Intelligence\". In 1957, [REDACTED] \n",
      "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(scrub(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False 3 ORG\n",
      "is be VERB VBZ aux xx True False 2 \n",
      "looking look VERB VBG ROOT xxxx True False 2 \n",
      "at at ADP IN prep xx True False 2 \n",
      "buying buy VERB VBG pcomp xxxx True False 2 \n",
      "U.K. u.k. PROPN NNP compound X.X. False False 3 GPE\n",
      "startup startup NOUN NN dobj xxxx True False 2 \n",
      "for for ADP IN prep xxx True False 2 \n",
      "$ $ SYM $ quantmod $ False False 3 MONEY\n",
      "1 1 NUM CD compound d False False 1 MONEY\n",
      "billion billion NUM CD pobj xxxx True False 1 MONEY\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop, token.ent_iob, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Text Classification is Your New Secret Weapon](https://medium.com/@ageitgey/text-classification-is-your-new-secret-weapon-7ca4fad15788)\n",
    "There‚Äôs several reasons why treating text as a classification problem instead of as an understanding problem tends to work really well‚Ää‚Äî‚Ääeven when using relatively simple linear classification models.\n",
    "* First, people constantly create and evolve language. Especially in an online word full of memes and emoji, writing code to reliably parse tweets and user reviews is going to be pretty difficult.With text classification, the algorithm doesn‚Äôt care whether the user wrote standard English, an emoji, or a reference to Goku. The algorithm is looking for statistical relationships between input phrases and outputs. If writing ‡≤†_‡≤† correlates more heavily with 1-star and 2-star reviews, the algorithm will pick that up even though it has no idea what a ‚Äúlook of disapproval‚Äù emoticon is. The classifier can still figure out what characters mean in the context of where they appear and how often they contribute to a particular output.\n",
    "* Second, website users don‚Äôt always write in the specific language that you expect. An NLP pipeline trained to handle American English is going to fall apart if you give it German text. It‚Äôs also going to do poorly if your user decides to write their reviews with Cockney Rhyming Slang‚Ää‚Äî‚Ääwhich is still technically English. Again, a classification algorithm doesn‚Äôt care what language the text is in as long as it can at least break apart the text into separate words and measure the effects of those words. As long as you give the classifier enough training data to cover a wide range of possible English and German user reviews, it will learn to handle both just fine.\n",
    "* And finally, a big reason that text classification is so great is because it is fast. Because linear text classification algorithms are so simple (compared to more complex machine learning models like recurrent neural networks), they can be trained quickly. You can train a linear classifier with gigabytes of text in minutes on a regular laptop. You don‚Äôt even need any fancy hardware like a GPU. So even if you can get a slightly better accuracy score with a different machine learning algorithm, sometimes the tradeoff isn‚Äôt worth it. And research has shown that often the accuracy gap is nearly zero anyway.\n",
    "\n",
    "While text classification models are simple to set up, that‚Äôs not to say they are always easy to get working well. The big catch is that you need a lot of training data. If you don‚Äôt have enough training data to cover the wide range of the ways that people write things, the model won‚Äôt ever be very accurate. The more training data you can collect, the better the model will perform. The real art of applying text classification well is in finding clever ways of automatically collecting or creating training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the User Review Model with fastText\n",
    "* Step 1: Download Training Data\n",
    "* Step 2: Format and Pre-process Training Data\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This example code is written for Python 3.6+!\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7719967364503546\n",
      "0.4958919395754404\n",
      "0.07558259520866972\n",
      "0.854002715151151\n",
      "0.2835488011913645\n",
      "0.7735036047128253\n",
      "0.2088045881349888\n",
      "0.5643298567941221\n",
      "0.995652148922587\n",
      "0.7461800415963821\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews_data = Path(\"dataset\") / \"review.json\"\n",
    "#fasttext_data = Path(\"fasttext_dataset.txt\")\n",
    "\n",
    "reviews_data = open('/Users/song/GoogleDrive/GitHub/NLP/yelp_dataset/yelp_academic_dataset_review.json', 'r')\n",
    "#fasttext_data = open('/Users/song/GoogleDrive/GitHub/NLP/yelp_dataset/fasttext_dataset.txt', 'w')\n",
    "\n",
    "training_data = open('/Users/song/GoogleDrive/GitHub/NLP/yelp_dataset/training_data.txt', 'w')\n",
    "test_data = open('/Users/song/GoogleDrive/GitHub/NLP/yelp_dataset/test_data.txt', 'w')\n",
    "\n",
    "# What percent of data to save separately as test data\n",
    "percent_test_data = 0.10\n",
    "\n",
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"([.!?,'/()])\", r\" \\1 \", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with reviews_data as input, \\\n",
    "    training_data as train_output, \\\n",
    "    test_data as test_output:\n",
    "    \n",
    "    for line in input:\n",
    "        review_data = json.loads(line)\n",
    "        rating = review_data['stars']\n",
    "        text = review_data['text'].replace(\"\\n\", \" \")\n",
    "        text = strip_formatting(text)\n",
    "\n",
    "        fasttext_line = \"__label__{} {}\".format(rating, text)\n",
    "\n",
    "        if random.random() <= percent_test_data:\n",
    "            test_output.write(fasttext_line + \"\\n\")\n",
    "        else:\n",
    "            train_output.write(fasttext_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: fasttext: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!fasttext supervised -input training_data.txt -output reviews_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_txt = open('/Users/song/GoogleDrive/GitHub/NLP/yelp_dataset/fasttext_dataset.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 The pizza was okay. Not the best I've had. I prefer Biaggio's on Flamingo / Fort Apache. The chef there can make a MUCH better NY style pizza. The pizzeria @ Cosmo was over priced for the quality and lack of personality in the food. Biaggio's is a much better pick if youre going for italian - family owned, home made recipes, people that actually CARE if you like their food. You dont get that at a pizzeria in a casino. I dont care what you say...\n",
      "\n",
      "__label__5 I love this place! My fiance And I go here atleast once a week. The portions are huge! Food is amazing. I love their carne asada. They have great lunch specials... Leticia is super nice and cares about what you think of her restaurant. You have to try their cheese enchiladas too the sauce is different And amazing!!!\n",
      "\n",
      "__label__1 Terrible. Dry corn bread. Rib tips were all fat and mushy and had no flavor. If you want bbq in this neighborhood go to john mulls roadkill grill. Trust me.\n",
      "\n",
      "__label__2 Back in 2005-2007 this place was my FAVORITE thai place EVER. I'd go here ALLLLL the time. I never had any complaints. Once they started to get more known and got busy, their service started to suck and their portion sizes got cut in half. I have a huge problem with paying MORE for way less food. The last time I went there I had the Pork Pad se Ew and it tasted good, but I finished my plate and was still hungry. I used to know the manager here and she would greet me with a \"Hello Melissa, nice to see you again, diet coke & pad thai or pad se ew?\" Now a days, I know she still knows me but she disregards my presence. Also, I had asked her what was up with the new portion sizes and she had no answer for me. Great food but not worth the money. I havent been back in over a year because I refuse to pay $10-15 for dinner and still be hungry after. Sorry PinKaow, you are not what you used to be!!\n",
      "\n",
      "__label__5 Delicious healthy food. The steak is amazing. Fish and pork are awesome too. Service is above and beyond. Not a bad thing to say about this place. Worth every penny!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(rating_txt.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n*** ESR (SGD 3YR REGS) - IPG 6.75% AREA ***\\r\\n \\r\\nNOT FOR PUBLICATION OR DISTRIBUTION, DIRECTLY OR INDIRECTLY, IN OR INTO THE UNITED STATES OR IN ANY JURISDICTION IN WHICH SUCH PUBLICATION OR DISTRIBUTION WOULD BE PROHIBITED BY APPLICABLE LAW.\\r\\n \\r\\nISSUER:              ESR Cayman Limited (‚ÄúESR‚Äù)\\r\\nOFFERING:            S$ fixed rate notes (the ‚ÄúNotes‚Äù); drawdown from US$2bn                         Multicurrency Debt Issuance Programme         \\r\\nRATINGS:             Unrated\\r\\nFORMAT:              Reg S (Cat 1)\\r\\nISSUE SIZE:          S$ Benchmark\\r\\nTENOR:               3-year\\r\\nINITIAL PX GUIDANCE: 6.75% area\\r\\nCOUPON:              Payable semi-annually in arrear\\r\\nSTATUS:              Subordinated, only for so long as the US$300m fixed rate                        Notes due 18 November 2019 issued by ESR on 18 November                         2016 (\"The Hana Notes) remain outstanding. Senior, from                         and including the date the Hana Notes are redeemed in full                      or are no longer outstanding\\r\\nUSE OF PROCEEDS:     Refinancing of existing borrowings, financing of potential                      acquisition, investment opportunities, working capital                          requirements and general corporate purposes\\r\\nOTHER DETAILS:       SGD250k denominations / English Law, subordination                              provisions under Cayman Law / CDP clearing /SGX-ST listing\\r\\n\\r\\nLIMITATION ON FINANCIAL INDEBTEDNESS: For so long as the Hana Notes remain outstanding, subject to certain exceptions, ESR shall not incur any additional Financial Indebtedness, unless such Financial Indebtedness ranks pari passu with, or is expressly subordinated to the Notes\\r\\n\\r\\nISSUER CALL OPTIONS: (i) Redemption for Taxation Reasons and (ii) Redemption in the case of Minimum Outstanding Amount (outstanding Notes being less than 10% of the aggregate principal amount of Notes issued)\\r\\n\\r\\nJGCs:                Credit Suisse, Bank of America Merrill Lynch,                                   DBS Bank Ltd., Standard Chartered Bank, UOB\\r\\nJOINT BOOKRUNNERS:   Credit Suisse (B&D), Bank of America Merrill Lynch,                             DBS Bank Ltd., Standard Chartered Bank, UOB, UBS\\r\\nTIMING:              As early as today\\'s business\\r\\nDEALROADSHOW:        www.dealroadshow.com | Passcode: ESR2019 (case-sensitive)|                      Direct link: https://dealroadshow.com/?ec=ESR2019\\r\\nPB CONCESSION:       25 cents\\r\\n \\r\\n\\r\\nThe securities referred to herein have not been and will not be registered under the U.S. Securities Act of 1933 (as amended) (the ‚ÄúSecurities Act‚Äù) or with any securities regulatory authority of any state of the United States or any other jurisdiction, and may not be offered, sold or delivered in the United States absent registration or an exemption from the registration requirements of the Securities Act and applicable state securities law. This announcement does not constitute nor form part of any offer or invitation to sell, issue or subscribe for securities in the United States or elsewhere where such offer or sale would be unlawful. No public offering of any securities is being or will be made in the United States or in any other jurisdiction where such an offering is restricted or prohibited, and ESR does not intend to register any part of the proposed offering of securities in the United States.\\r\\n \\r\\nThis notice is for information purposes only and does not constitute an offer or sale of securities in the United States or any other jurisdiction. Neither this notice nor any portion hereof may be sent or transmitted into the United States or any jurisdiction where to do so is unlawful. Any failure to comply with these restrictions may constitute a violation of the United States securities law or the applicable laws of any such other jurisdiction.\\r\\n \\r\\nThe securities may not be offered or sold, or be made the subject of an invitation for subscription or purchase, whether directly or indirectly, to persons in Singapore other than (i) to an institutional investor under Section 274 of the Securities and Futures Act, Chapter 289 of Singapore (the \"SFA\"), (ii) to a relevant person pursuant to Section 275(1), or any person pursuant to Section 275(1A), and in accordance with the conditions specified in Section 275, of the SFA or (iii) otherwise pursuant to, and in accordance with the conditions of, any other applicable provision of the SFA.\\r\\n \\r\\nSection 309B Notification: The securities referred to herein are prescribed capital markets products (as defined in the Securities and Futures (Capital Markets Products) Regulations 2018 of Singapore) and Excluded Investment Products (as defined in MAS Notice SFA 04-N12: Notice on the Sale of Investment Products and MAS Notice FAA-N16: Notice on Recommendations on Investment Products).\\r\\n \\r\\nMiFID II professionals/ECPs-only /No PRIIPs KID ‚Äì Target market (MiFID II product governance) is expected to be eligible counterparties and professional clients only (all distribution channels). No PRIIPs key information document (KID) has been prepared as not available to retail in EEA.\\r\\n\\r\\n\\r\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_str = '\\r\\n*** ESR (SGD 3YR REGS) - IPG 6.75% AREA ***\\r\\n \\r\\nNOT FOR PUBLICATION OR DISTRIBUTION, DIRECTLY OR INDIRECTLY, IN OR INTO THE UNITED STATES OR IN ANY JURISDICTION IN WHICH SUCH PUBLICATION OR DISTRIBUTION WOULD BE PROHIBITED BY APPLICABLE LAW.\\r\\n \\r\\nISSUER:              ESR Cayman Limited (‚ÄúESR‚Äù)\\r\\nOFFERING:            S$ fixed rate notes (the ‚ÄúNotes‚Äù); drawdown from US$2bn                         Multicurrency Debt Issuance Programme         \\r\\nRATINGS:             Unrated\\r\\nFORMAT:              Reg S (Cat 1)\\r\\nISSUE SIZE:          S$ Benchmark\\r\\nTENOR:               3-year\\r\\nINITIAL PX GUIDANCE: 6.75% area\\r\\nCOUPON:              Payable semi-annually in arrear\\r\\nSTATUS:              Subordinated, only for so long as the US$300m fixed rate                        Notes due 18 November 2019 issued by ESR on 18 November                         2016 (\"The Hana Notes) remain outstanding. Senior, from                         and including the date the Hana Notes are redeemed in full                      or are no longer outstanding\\r\\nUSE OF PROCEEDS:     Refinancing of existing borrowings, financing of potential                      acquisition, investment opportunities, working capital                          requirements and general corporate purposes\\r\\nOTHER DETAILS:       SGD250k denominations / English Law, subordination                              provisions under Cayman Law / CDP clearing /SGX-ST listing\\r\\n\\r\\nLIMITATION ON FINANCIAL INDEBTEDNESS: For so long as the Hana Notes remain outstanding, subject to certain exceptions, ESR shall not incur any additional Financial Indebtedness, unless such Financial Indebtedness ranks pari passu with, or is expressly subordinated to the Notes\\r\\n\\r\\nISSUER CALL OPTIONS: (i) Redemption for Taxation Reasons and (ii) Redemption in the case of Minimum Outstanding Amount (outstanding Notes being less than 10% of the aggregate principal amount of Notes issued)\\r\\n\\r\\nJGCs:                Credit Suisse, Bank of America Merrill Lynch,                                   DBS Bank Ltd., Standard Chartered Bank, UOB\\r\\nJOINT BOOKRUNNERS:   Credit Suisse (B&D), Bank of America Merrill Lynch,                             DBS Bank Ltd., Standard Chartered Bank, UOB, UBS\\r\\nTIMING:              As early as today\\'s business\\r\\nDEALROADSHOW:        www.dealroadshow.com | Passcode: ESR2019 (case-sensitive)|                      Direct link: https://dealroadshow.com/?ec=ESR2019\\r\\nPB CONCESSION:       25 cents\\r\\n \\r\\n\\r\\nThe securities referred to herein have not been and will not be registered under the U.S. Securities Act of 1933 (as amended) (the ‚ÄúSecurities Act‚Äù) or with any securities regulatory authority of any state of the United States or any other jurisdiction, and may not be offered, sold or delivered in the United States absent registration or an exemption from the registration requirements of the Securities Act and applicable state securities law. This announcement does not constitute nor form part of any offer or invitation to sell, issue or subscribe for securities in the United States or elsewhere where such offer or sale would be unlawful. No public offering of any securities is being or will be made in the United States or in any other jurisdiction where such an offering is restricted or prohibited, and ESR does not intend to register any part of the proposed offering of securities in the United States.\\r\\n \\r\\nThis notice is for information purposes only and does not constitute an offer or sale of securities in the United States or any other jurisdiction. Neither this notice nor any portion hereof may be sent or transmitted into the United States or any jurisdiction where to do so is unlawful. Any failure to comply with these restrictions may constitute a violation of the United States securities law or the applicable laws of any such other jurisdiction.\\r\\n \\r\\nThe securities may not be offered or sold, or be made the subject of an invitation for subscription or purchase, whether directly or indirectly, to persons in Singapore other than (i) to an institutional investor under Section 274 of the Securities and Futures Act, Chapter 289 of Singapore (the \"SFA\"), (ii) to a relevant person pursuant to Section 275(1), or any person pursuant to Section 275(1A), and in accordance with the conditions specified in Section 275, of the SFA or (iii) otherwise pursuant to, and in accordance with the conditions of, any other applicable provision of the SFA.\\r\\n \\r\\nSection 309B Notification: The securities referred to herein are prescribed capital markets products (as defined in the Securities and Futures (Capital Markets Products) Regulations 2018 of Singapore) and Excluded Investment Products (as defined in MAS Notice SFA 04-N12: Notice on the Sale of Investment Products and MAS Notice FAA-N16: Notice on Recommendations on Investment Products).\\r\\n \\r\\nMiFID II professionals/ECPs-only /No PRIIPs KID ‚Äì Target market (MiFID II product governance) is expected to be eligible counterparties and professional clients only (all distribution channels). No PRIIPs key information document (KID) has been prepared as not available to retail in EEA.\\r\\n\\r\\n\\r\\n'\n",
    "\n",
    "email_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [`4.Dive Into NLTK, Part V: Using Stanford Text Analysis Tools in Python`](https://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python)<a name=4></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
